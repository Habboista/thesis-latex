@INPROCEEDINGS{CAM,
  author={Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Learning Deep Features for Discriminative Localization}, 
  year={2016},
  volume={},
  number={},
  pages={2921-2929},
  keywords={Visualization;Neural networks;Training;Object recognition;Computer vision;Detectors;Spatial resolution},
  doi={10.1109/CVPR.2016.319}}

@INPROCEEDINGS{GradCAM,
  author={Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
  booktitle={2017 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization}, 
  year={2017},
  volume={},
  number={},
  pages={618-626},
  keywords={Visualization;Cats;Dogs;Computer architecture;Knowledge discovery},
  doi={10.1109/ICCV.2017.74}}

@Article{DL_overview,
author={Sarker, Iqbal H.},
title={Deep Learning: A Comprehensive Overview on Techniques, Taxonomy, Applications and Research Directions},
journal={SN Computer Science},
year={2021},
month={Aug},
day={18},
volume={2},
number={6},
pages={420},
abstract={Deep learning (DL), a branch of machine learning (ML) and artificial intelligence (AI) is nowadays considered as a core technology of today's Fourth Industrial Revolution (4IR or Industry 4.0). Due to its learning capabilities from data, DL technology originated from artificial neural network (ANN), has become a hot topic in the context of computing, and is widely applied in various application areas like healthcare, visual recognition, text analytics, cybersecurity, and many more. However, building an appropriate DL model is a challenging task, due to the dynamic nature and variations in real-world problems and data. Moreover, the lack of core understanding turns DL methods into black-box machines that hamper development at the standard level. This article presents a structured and comprehensive view on DL techniques including a taxonomy considering various types of real-world tasks like supervised or unsupervised. In our taxonomy, we take into account deep networks for supervised or discriminative learning, unsupervised or generative learning as well as hybrid learning and relevant others. We also summarize real-world application areas where deep learning techniques can be used. Finally, we point out ten potential aspects for future generation DL modeling with research directions. Overall, this article aims to draw a big picture on DL modeling that can be used as a reference guide for both academia and industry professionals.},
issn={2661-8907},
doi={10.1007/s42979-021-00815-1},
url={https://doi.org/10.1007/s42979-021-00815-1}
}

@Article{Zablocki2022,
author={Zablocki, {\'E}loi
and Ben-Younes, H{\'e}di
and P{\'e}rez, Patrick
and Cord, Matthieu},
title={Explainability of Deep Vision-Based Autonomous Driving Systems: Review and Challenges},
journal={International Journal of Computer Vision},
year={2022},
month={Oct},
day={01},
volume={130},
number={10},
pages={2425-2452},
abstract={This survey reviews explainability methods for vision-based self-driving systems trained with behavior cloning. The concept of explainability has several facets and the need for explainability is strong in driving, a safety-critical application. Gathering contributions from several research fields, namely computer vision, deep learning, autonomous driving, explainable AI (X-AI), this survey tackles several points. First, it discusses definitions, context, and motivation for gaining more interpretability and explainability from self-driving systems, as well as the challenges that are specific to this application. Second, methods providing explanations to a black-box self-driving system in a post-hoc fashion are comprehensively organized and detailed. Third, approaches from the literature that aim at building more interpretable self-driving systems by design are presented and discussed in detail. Finally, remaining open-challenges and potential future research directions are identified and examined.},
issn={1573-1405},
doi={10.1007/s11263-022-01657-x},
url={https://doi.org/10.1007/s11263-022-01657-x}
}

@ARTICLE{XAI_industry,
  author={Ahmed, Imran and Jeon, Gwanggil and Piccialli, Francesco},
  journal={IEEE Transactions on Industrial Informatics}, 
  title={From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where}, 
  year={2022},
  volume={18},
  number={8},
  pages={5031-5042},
  keywords={Fourth Industrial Revolution;Artificial intelligence;Industries;Hidden Markov models;Manufacturing;Service robots;Robots;Artificial intelligence (AI);cloud computing;cyber-physical system;explainable artificial intelligence (XAI);Industry 4.0;Internet of Things (IoT)},
  doi={10.1109/TII.2022.3146552}}

@Article{XAI_healthcare,
AUTHOR = {Hulsen, Tim},
TITLE = {Explainable Artificial Intelligence (XAI): Concepts and Challenges in Healthcare},
JOURNAL = {AI},
VOLUME = {4},
YEAR = {2023},
NUMBER = {3},
PAGES = {652--666},
URL = {https://www.mdpi.com/2673-2688/4/3/34},
ISSN = {2673-2688},
ABSTRACT = {Artificial Intelligence (AI) describes computer systems able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and language translation. Examples of AI techniques are machine learning, neural networks, and deep learning. AI can be applied in many different areas, such as econometrics, biometry, e-commerce, and the automotive industry. In recent years, AI has found its way into healthcare as well, helping doctors make better decisions (“clinical decision support”), localizing tumors in magnetic resonance images, reading and analyzing reports written by radiologists and pathologists, and much more. However, AI has one big risk: it can be perceived as a “black box”, limiting trust in its reliability, which is a very big issue in an area in which a decision can mean life or death. As a result, the term Explainable Artificial Intelligence (XAI) has been gaining momentum. XAI tries to ensure that AI algorithms (and the resulting decisions) can be understood by humans. In this narrative review, we will have a look at some central concepts in XAI, describe several challenges around XAI in healthcare, and discuss whether it can really help healthcare to advance, for example, by increasing understanding and trust. Finally, alternatives to increase trust in AI are discussed, as well as future research possibilities in the area of XAI.},
DOI = {10.3390/ai4030034}
}

@article{Lipton,
author = {Lipton, Zachary C.},
title = {The mythos of model interpretability},
year = {2018},
issue_date = {October 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {61},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/3233231},
doi = {10.1145/3233231},
abstract = {In machine learning, the concept of interpretability is both important and slippery.},
journal = {Commun. ACM},
month = {sep},
pages = {36-43},
numpages = {8}
}

@article{XAI_review,
author = {Minh, Dang and Wang, H. Xiang and Li, Y. Fen and Nguyen, Tan N.},
title = {Explainable artificial intelligence: a comprehensive review},
year = {2022},
issue_date = {Jun 2022},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {55},
number = {5},
issn = {0269-2821},
url = {https://doi.org/10.1007/s10462-021-10088-y},
doi = {10.1007/s10462-021-10088-y},
abstract = {Thanks to the exponential growth in computing power and vast amounts of data, artificial intelligence (AI) has witnessed remarkable developments in recent years, enabling it to be ubiquitously adopted in our daily lives. Even though AI-powered systems have brought competitive advantages, the black-box nature makes them lack transparency and prevents them from explaining their decisions. This issue has motivated the introduction of explainable artificial intelligence (XAI), which promotes AI algorithms that can show their internal process and explain how they made decisions. The number of XAI research has increased significantly in recent years, but there lacks a unified and comprehensive review of the latest XAI progress. This review aims to bridge the gap by discovering the critical perspectives of the rapidly growing body of research associated with XAI. After offering the readers a solid XAI background, we analyze and review various XAI methods, which are grouped into (i) pre-modeling explainability, (ii) interpretable model, and (iii) post-modeling explainability. We also pay attention to the current methods that dedicate to interpret and analyze deep learning methods. In addition, we systematically discuss various XAI challenges, such as the trade-off between the performance and the explainability, evaluation methods, security, and policy. Finally, we show the standard approaches that are leveraged to deal with the mentioned challenges.},
journal = {Artif. Intell. Rev.},
month = {jun},
pages = {3503-3568},
numpages = {66},
keywords = {Explainable artificial intelligence, Interpretability, Black-box models, Deep learning, Machine learning}
}

@article{Miller,
title = {Explanation in artificial intelligence: Insights from the social sciences},
journal = {Artificial Intelligence},
volume = {267},
pages = {1-38},
year = {2019},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2018.07.007},
url = {https://www.sciencedirect.com/science/article/pii/S0004370218305988},
author = {Tim Miller},
keywords = {Explanation, Explainability, Interpretability, Explainable AI, Transparency},
abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a ‘good’ explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.}
}

@inproceedings{examples_enough,
author = {Kim, Been and Khanna, Rajiv and Koyejo, Oluwasanmi},
title = {Examples are not enough, learn to criticize! criticism for interpretability},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {2288-2296},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

@INPROCEEDINGS{Dijk,
  author={Van Dijk, Tom and De Croon, Guido},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={How Do Neural Networks See Depth in Single Images?}, 
  year={2019},
  volume={},
  number={},
  pages={2183-2191},
  keywords={Cameras;Estimation;Training;Automobiles;Visualization;Biological neural networks},
  doi={10.1109/ICCV.2019.00227}}

@INPROCEEDINGS{towards_interpretable,
  author={You, Zunzhi and Tsai, Yi-Hsuan and Chiu, Wei-Chen and Li, Guanbin},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Towards Interpretable Deep Networks for Monocular Depth Estimation}, 
  year={2021},
  volume={},
  number={},
  pages={12859-12868},
  keywords={Analytical models;Visualization;Computer vision;Codes;Annotations;Computer network reliability;Semantics;3D from a single image and shape-from-x;Explainable AI},
  doi={10.1109/ICCV48922.2021.01264}}

@INPROCEEDINGS{Hu,
  author={Hu, Junjie and Zhang, Yan and Okatani, Takayuki},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Visualization of Convolutional Neural Networks for Monocular Depth Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={3868-3877},
  keywords={Visualization;Estimation;Optimization;Task analysis;Training;Object recognition;Convolutional neural networks},
  doi={10.1109/ICCV.2019.00397}}

@INPROCEEDINGS{9157111,
  author={Xu, Yiran and Yang, Xiaoyin and Gong, Lihang and Lin, Hsuan-Chu and Wu, Tz-Ying and Li, Yunsheng and Vasconcelos, Nuno},
  booktitle={2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Explainable Object-Induced Action Decision for Autonomous Vehicles}, 
  year={2020},
  volume={},
  number={},
  pages={9520-9529},
  keywords={Autonomous vehicles;Automobiles;Cognition;Three-dimensional displays;Visualization;Turning;Task analysis},
  doi={10.1109/CVPR42600.2020.00954}}

@InProceedings{Deconvnet,
author="Zeiler, Matthew D.
and Fergus, Rob",
editor="Fleet, David
and Pajdla, Tomas
and Schiele, Bernt
and Tuytelaars, Tinne",
title="Visualizing and Understanding Convolutional Networks",
booktitle="Computer Vision -- ECCV 2014",
year="2014",
publisher="Springer International Publishing",
address="Cham",
pages="818--833",
abstract="Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.",
isbn="978-3-319-10590-1"
}

@inproceedings{LIME,
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
title = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
year = {2016},
isbn = {9781450342322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2939672.2939778},
doi = {10.1145/2939672.2939778},
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one.In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
pages = {1135–1144},
numpages = {10},
keywords = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
location = {San Francisco, California, USA},
series = {KDD '16}
}

##############################################################
#                        BOOKS
##############################################################
@book{Kahneman,
  added-at = {2016-11-26T13:19:29.000+0100},
  author = {Kahneman, Daniel},
  biburl = {https://www.bibsonomy.org/bibtex/2c6ab2fa616e40cc280ac0c905eac02a0/machinelearning},
  date-added = {2015-03-16 15:37:14 +0000},
  date-modified = {2015-03-16 15:37:14 +0000},
  interhash = {a1400a299a00de009ec8eda73e6289af},
  intrahash = {c6ab2fa616e40cc280ac0c905eac02a0},
  keywords = {imported ml},
  publisher = {Allen Lane},
  timestamp = {2016-11-26T13:20:49.000+0100},
  title = {Thinking, Fast and Slow},
  year = 2011
}


@book{ML_book,
author = {Shalev-Shwartz, Shai and Ben-David, Shai},
title = {Understanding Machine Learning: From Theory to Algorithms},
year = {2014},
isbn = {1107057132},
publisher = {Cambridge University Press},
address = {USA},
abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.}
}

@book{pinker2009mind,
  title={How the Mind Works},
  author={Pinker, S.},
  isbn={9780393334777},
  lccn={97001855},
  url={https://books.google.it/books?id=48zpLNAmTXwC},
  year={2009},
  publisher={W. W. Norton}
}

@book{molnar2022,
  title      = {Interpretable Machine Learning},
  author     = {Christoph Molnar},
  year       = {2022},
  subtitle   = {A Guide for Making Black Box Models Explainable},
  edition    = {2},
  url        = {https://christophm.github.io/interpretable-ml-book}
}

@book{multiview,
    place={Cambridge},
    edition={2},
    title={Multiple View Geometry in Computer Vision},
    publisher={Cambridge University Press},
    author={Hartley, Richard and Zisserman, Andrew},
    year={2004}
}

@book{computer_vision,
    edition={2},
    title="Computer Vision",
    publisher="Springer Cham",
    author="Richard Szeliski",
    year={2011}
}

##############################################################
#                        CLASSIC
##############################################################

@inproceedings{saxena1,
author = {Michels, Jeff and Saxena, Ashutosh and Ng, Andrew Y.},
title = {High speed obstacle avoidance using monocular vision and reinforcement learning},
year = {2005},
isbn = {1595931805},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1102351.1102426},
doi = {10.1145/1102351.1102426},
abstract = {We consider the task of driving a remote control car at high speeds through unstructured outdoor environments. We present an approach in which supervised learning is first used to estimate depths from single monocular images. The learning algorithm can be trained either on real camera images labeled with ground-truth distances to the closest obstacles, or on a training set consisting of synthetic graphics images. The resulting algorithm is able to learn monocular vision cues that accurately estimate the relative depths of obstacles in a scene. Reinforcement learning/policy search is then applied within a simulator that renders synthetic scenes. This learns a control policy that selects a steering direction as a function of the vision system's output. We present results evaluating the predictive ability of the algorithm both on held out test data, and in actual autonomous driving experiments.},
booktitle = {Proceedings of the 22nd International Conference on Machine Learning},
pages = {593-600},
numpages = {8},
location = {Bonn, Germany},
series = {ICML '05}
}

@inproceedings{saxena2,
author = {Saxena, Ashutosh and Chung, Sung H. and Ng, Andrew Y.},
title = {Learning depth from single monocular images},
year = {2005},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We consider the task of depth estimation from a single monocular image. We take a supervised learning approach to this problem, in which we begin by collecting a training set of monocular images (of unstructured outdoor environments which include forests, trees, buildings, etc.) and their corresponding ground-truth depthmaps. Then, we apply supervised learning to predict the depthmap as a function of the image. Depth estimation is a challenging problem, since local features alone are insufficient to estimate depth at a point, and one needs to consider the global context of the image. Our model uses a discriminatively-trained Markov Random Field (MRF) that incorporates multiscale local- and global-image features, and models both depths at individual points as well as the relation between depths at different points. We show that, even on unstructured scenes, our algorithm is frequently able to recover fairly accurate depthmaps.},
booktitle = {Proceedings of the 18th International Conference on Neural Information Processing Systems},
pages = {1161-1168},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'05}
}

@inproceedings{saxena3,
author = {Saxena, Ashutosh and Schulte, Jamie and Ng, Andrew Y.},
title = {Depth estimation using monocular and stereo cues},
year = {2007},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
abstract = {Depth estimation in computer vision and robotics is most commonly done via stereo vision (stereopsis), in which images from two cameras are used to triangulate and estimate distances. However, there are also numerous monocular visual cues--such as texture variations and gradients, defocus, color/haze, etc. --that have heretofore been little exploited in such systems. Some of these cues apply even in regions without texture, where stereo would work poorly. In this paper, we apply a Markov Random Field (MRF) learning algorithm to capture some of these monocular cues, and incorporate them into a stereo system. We show that by adding monocular cues to stereo (triangulation) ones, we obtain significantly more accurate depth estimates than is possible using either monocular or stereo cues alone. This holds true for a large variety of environments, including both indoor environments and unstructured outdoor environments containing trees/forests, buildings, etc. Our approach is general, and applies to incorporating monocular cues together with any off-the-shelf stereo system.},
booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
pages = {2197-2203},
numpages = {7},
location = {Hyderabad, India},
series = {IJCAI'07}
}

@INPROCEEDINGS{saxena4,
  author={Saxena, Ashutosh and Sun, Min and Ng, Andrew Y.},
  booktitle={2007 IEEE 11th International Conference on Computer Vision}, 
  title={Learning 3-D Scene Structure from a Single Still Image}, 
  year={2007},
  volume={},
  number={},
  pages={1-8},
  keywords={Layout;Markov random fields;Supervised learning;Sun;Computer science;Inference algorithms;Art;Rendering (computer graphics);Internet;Humans},
  doi={10.1109/ICCV.2007.4408828}}

@ARTICLE{saxena5,
  author={Saxena, Ashutosh and Sun, Min and Ng, Andrew Y.},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Make3D: Learning 3D Scene Structure from a Single Still Image}, 
  year={2009},
  volume={31},
  number={5},
  pages={824-840},
  keywords={Layout;Image segmentation;Markov random fields;Computer vision;Sun;Supervised learning;Art;Rendering (computer graphics);Internet;Large-scale systems;Machine learning;monocular vision;learning depth;vision and scene understanding;scene analysis;depth cues.;Vision and Scene Understanding;Scene Analysis;Depth cues;Statistical;Computer vision;Virtual reality;Image-based rendering;Machine learning},
  doi={10.1109/TPAMI.2008.132}}


@INPROCEEDINGS{IM2CAD,
  author={Izadinia, Hamid and Shan, Qi and Seitz, Steven M.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={IM2CAD}, 
  year={2017},
  volume={},
  number={},
  pages={2422-2431},
  keywords={Solid modeling;Three-dimensional displays;Computational modeling;Databases;Estimation;Image reconstruction},
  doi={10.1109/CVPR.2017.260}
}

@ARTICLE{DepthTransfer,
  author={Karsch, Kevin and Liu, Ce and Kang, Sing Bing},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Depth Transfer: Depth Extraction from Video Using Non-Parametric Sampling}, 
  year={2014},
  volume={36},
  number={11},
  pages={2144-2158},
  keywords={Databases;Estimation;Optimization;Cameras;Optical imaging;Three-dimensional displays;Image reconstruction;Depth estimation;monocular depth;motion estimation;data-driven;2D-to-3D},
  doi={10.1109/TPAMI.2014.2316835}
}

@inproceedings{konrad1,
author = {J. Konrad and G. Brown and M. Wang and P. Ishwar and C. Wu and D. Mukherjee},
title = {{Automatic 2D-to-3D image conversion using 3D examples from the internet}},
volume = {8288},
booktitle = {Stereoscopic Displays and Applications XXIII},
editor = {Andrew J. Woods and Nicolas S. Holliman and Gregg E. Favalora},
organization = {International Society for Optics and Photonics},
publisher = {SPIE},
pages = {82880F},
keywords = {2D-to-3D conversion, 3D content generation, Stereoscopic 3D, YouTube 3D, SIFT flow, Cross-bilateral filtering},
year = {2012},
doi = {10.1117/12.910601},
URL = {https://doi.org/10.1117/12.910601}
}

@INPROCEEDINGS{konrad2,
  author={Konrad, Janusz and Wang, Meng and Ishwar, Prakash},
  booktitle={2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops}, 
  title={2D-to-3D image conversion by learning depth from examples}, 
  year={2012},
  volume={},
  number={},
  pages={16-22},
  keywords={Databases;Image edge detection;Cameras;Dictionaries;Smoothing methods;Measurement;Humans},
  doi={10.1109/CVPRW.2012.6238903}}

@ARTICLE{SIFTFlow,
  author={Liu, Ce and Yuen, Jenny and Torralba, Antonio},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={SIFT Flow: Dense Correspondence across Scenes and Its Applications}, 
  year={2011},
  volume={33},
  number={5},
  pages={978-994},
  keywords={Pixel;Optical imaging;Visualization;Databases;Belief propagation;Complexity theory;Object recognition;Scene alignment;dense scene correspondence;SIFT flow;coarse to fine;belief propagation;alignment-based large database framework;satellite image registration;face recognition;motion prediction for a single image;motion synthesis via object transfer.},
  doi={10.1109/TPAMI.2010.147}}


@article{autopopup1,
author = {Hoiem, Derek and Efros, Alexei A. and Hebert, Martial},
title = {Automatic photo pop-up},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1073204.1073232},
doi = {10.1145/1073204.1073232},
abstract = {This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children's pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: "ground", "sky", and "vertical". These labels are then used to "cut and fold" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person's photo album.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {577-584},
numpages = {8},
keywords = {single-view reconstruction, machine learning, image-based rendering, image segmentation}
}

@INPROCEEDINGS{autopopup2,
  author={Hoiem, D. and Efros, A.A. and Hebert, M.},
  booktitle={Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1}, 
  title={Geometric context from a single image}, 
  year={2005},
  volume={1},
  number={},
  pages={654-661 Vol. 1},
  keywords={Layout;Object detection;Computer vision;Surface reconstruction;Cameras;Application software;Image reconstruction;Humans;Solid modeling;Robustness},
  doi={10.1109/ICCV.2005.107}}

@INPROCEEDINGS{autopopup3,
  author={Hoiem, D. and Efros, A.A. and Hebert, M.},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
  title={Putting Objects in Perspective}, 
  year={2006},
  volume={2},
  number={},
  pages={2137-2144},
  keywords={Layout;Object detection;Cameras;Roads;Context modeling;Geometry;Detectors;Computer vision;Robot kinematics;Solid modeling},
  doi={10.1109/CVPR.2006.232}}

@INPROCEEDINGS{autopopup4,
  author={Hoiem, Derek and Stein, Andrew N. and Efros, Alexei A. and Hebert, Martial},
  booktitle={2007 IEEE 11th International Conference on Computer Vision}, 
  title={Recovering Occlusion Boundaries from a Single Image}, 
  year={2007},
  volume={},
  number={},
  pages={1-8},
  keywords={Layout;Computer vision;Humans;Vegetation mapping;Robot vision systems;Navigation;Image segmentation;Displays;Image processing;Biomedical imaging},
  doi={10.1109/ICCV.2007.4408985}}

@Article{autopopup5,
author={Hoiem, Derek
and Efros, Alexei A.
and Hebert, Martial},
title={Recovering Surface Layout from an Image},
journal={International Journal of Computer Vision},
year={2007},
month={Oct},
day={01},
volume={75},
number={1},
pages={151-172},
abstract={Humans have an amazing ability to instantly grasp the overall 3D structure of a scene---ground orientation, relative positions of major landmarks, etc.---even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this ``surface layout'' of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis.},
issn={1573-1405},
doi={10.1007/s11263-006-0031-y},
url={https://doi.org/10.1007/s11263-006-0031-y}
}


@INPROCEEDINGS{autopopup6,
  author={Hoiem, Derek and Efros, Alexei A. and Hebert, Martial},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Closing the loop in scene interpretation}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  keywords={Layout;Image analysis;Surface reconstruction;Feedforward systems;Object detection;Inference algorithms;Robot kinematics;Cameras;Object recognition;Reflectivity},
  doi={10.1109/CVPR.2008.4587587}}

@InProceedings{VideoCompass,
author="Ko{\v{s}}eck{\'a}, Jana
and Zhang, Wei",
editor="Heyden, Anders
and Sparr, Gunnar
and Nielsen, Mads
and Johansen, Peter",
title="Video Compass",
booktitle="Computer Vision --- ECCV 2002",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="476--490",
abstract="In this paper we describe a flexible approach for determining the relative orientation of the camera with respect to the scene. The main premise of the approach is the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points, which provide strong constraints on camera parameters and relative orientation of the camera with respect to the scene.",
isbn="978-3-540-47979-6"
}


##############################################################
#                        SL
##############################################################
@INPROCEEDINGS{360MonoDepth,
  author={Rey-Area, Manuel and Yuan, Mingze and Richardt, Christian},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={360MonoDepth: High-Resolution 360° Monocular Depth Estimation}, 
  year={2022},
  volume={},
  number={},
  pages={3752-3762},
  keywords={Computer vision;Image resolution;Three-dimensional displays;Codes;Estimation;Graphics processing units;Virtual reality;3D from single images},
  doi={10.1109/CVPR52688.2022.00374}}


@INPROCEEDINGS{BoostingDepth,
  author={Miangoleh, S. Mahdi H. and Dille, Sebastian and Mai, Long and Paris, Sylvain and Aksoy, Yağız},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging}, 
  year={2021},
  volume={},
  number={},
  pages={9680-9689},
  keywords={Location awareness;Image segmentation;Computer vision;Image resolution;Merging;Neural networks;Estimation},
  doi={10.1109/CVPR46437.2021.00956},
}

@ARTICLE{depth_as_classification,
  author={Cao, Yuanzhouhan and Wu, Zifeng and Shen, Chunhua},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Estimating Depth From Monocular Images as Classification Using Deep Fully Convolutional Residual Networks}, 
  year={2018},
  volume={28},
  number={11},
  pages={3174-3182},
  keywords={Estimation;Training;Semantics;Network architecture;Predictive models;Neural networks;Probability distribution;Classification;deep residual networks;depth estimation},
  doi={10.1109/TCSVT.2017.2740321}
}

@INPROCEEDINGS{ordinal_regression,
  author={Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Deep Ordinal Regression Network for Monocular Depth Estimation}, 
  year={2018},
  volume={},
  number={},
  pages={2002-2011},
  keywords={Estimation;Feature extraction;Training;Spatial resolution;Kernel;Three-dimensional displays;Two dimensional displays},
  doi={10.1109/CVPR.2018.00214}
}

@INPROCEEDINGS{AdaBins,
  author={Farooq Bhat, Shariq and Alhashim, Ibraheem and Wonka, Peter},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={AdaBins: Depth Estimation Using Adaptive Bins}, 
  year={2021},
  volume={},
  number={},
  pages={4008-4017},
  keywords={Measurement;Image segmentation;Three-dimensional displays;Image resolution;Estimation;Computer architecture;Transformers},
  doi={10.1109/CVPR46437.2021.00400},
  url={https://github.com/isl-org/ZoeDepth},
}

@InProceedings{LocalBins,
    author="Bhat, Shariq Farooq
        and Alhashim, Ibraheem
        and Wonka, Peter",
    editor="Avidan, Shai
        and Brostow, Gabriel
        and Ciss{\'e}, Moustapha
        and Farinella, Giovanni Maria
        and Hassner, Tal",
    title="LocalBins: Improving Depth Estimation by Learning Local Distributions",
    booktitle="Computer Vision -- ECCV 2022",
    year="2022",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="480--496",
    abstract="We propose a novelarchitecture for depth estimation from a single image. The architecture itself is based on the popular encoder-decoder architecture that is frequently used as a starting point for all dense regression tasks. We build on AdaBins which estimates a global distribution of depth values for the input image and evolve the architecture in two ways. First, instead of predicting global depth distributions, we predict depth distributions of local neighborhoods at every pixel. Second, instead of predicting depth distributions only towards the end of the decoder, we involve all layers of the decoder. We call this new architecture LocalBins. Our results demonstrate a clear improvement over the state-of-the-art in all metrics on the NYU-Depth V2 dataset. Code and pretrained models will be made publicly available (https://github.com/shariqfarooq123/LocalBins).",
    isbn="978-3-031-19769-7",
    url={https://github.com/shariqfarooq123/LocalBins},
}


@misc{ZoeDepth,
  doi = {10.48550/ARXIV.2302.12288},
  url = {https://arxiv.org/abs/2302.12288},
  author = {Bhat, Shariq Farooq and Birkl, Reiner and Wofk, Diana and Wonka, Peter and Müller, Matthias},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Eigen,
    author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Depth Map Prediction from a Single Image using a Multi-Scale Deep Network},
    url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf},
    volume = {27},
    year = {2014}
}

@INPROCEEDINGS{Eigen2,
  author={Eigen, David and Fergus, Rob},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture}, 
  year={2015},
  volume={},
  number={},
  pages={2650-2658},
  keywords={Semantics;Estimation;Labeling;Image segmentation;Adaptation models;Spatial resolution},
  doi={10.1109/ICCV.2015.304}
}

@INPROCEEDINGS{Laina,
  author={Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
  booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
  title={Deeper Depth Prediction with Fully Convolutional Residual Networks}, 
  year={2016},
  volume={},
  number={},
  pages={239-248},
  keywords={Estimation;Image resolution;Three-dimensional displays;Computer architecture;Computer vision;Semantics;Optimization;Depth prediction;CNN},
  doi={10.1109/3DV.2016.32}
}

@INPROCEEDINGS{Zoran,
  author={Zoran, Daniel and Isola, Phillip and Krishnan, Dilip and Freeman, William T.},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Learning Ordinal Relationships for Mid-Level Vision}, 
  year={2015},
  volume={},
  number={},
  pages={388-396},
  keywords={Measurement;Estimation;Image edge detection;Image decomposition;Context;Image segmentation;Marine vehicles},
  doi={10.1109/ICCV.2015.52},
  url={ https://dilipkay.wordpress.com/ordinals/}
}

@INPROCEEDINGS{denseViT,
  author={Ranftl, René and Bochkovskiy, Alexey and Koltun, Vladlen},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Vision Transformers for Dense Prediction}, 
  year={2021},
  volume={},
  number={},
  pages={12159-12168},
  keywords={Computer vision;Image resolution;Semantics;Neural networks;Estimation;Training data;Computer architecture;Machine learning architectures and formulations;3D from a single image and shape-from-x;Segmentation;grouping and shape},
  doi={10.1109/ICCV48922.2021.01196}
}

##############################################################
#                        DATASETS
##############################################################

@INPROCEEDINGS{KITTI,
  author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Are we ready for autonomous driving? The KITTI vision benchmark suite}, 
  year={2012},
  volume={},
  number={},
  pages={3354-3361},
  keywords={Benchmark testing;Cameras;Optical imaging;Visualization;Optical sensors;Measurement},
  doi={10.1109/CVPR.2012.6248074}
}

@InProceedings{NYUv2,
  author="Silberman, Nathan
    and Hoiem, Derek
    and Kohli, Pushmeet
    and Fergus, Rob",
  editor="Fitzgibbon, Andrew
    and Lazebnik, Svetlana
    and Perona, Pietro
    and Sato, Yoichi
    and Schmid, Cordelia",
  title="Indoor Segmentation and Support Inference from RGBD Images",
  booktitle="Computer Vision -- ECCV 2012",
  year="2012",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="746--760",
  abstract="We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.",
  isbn="978-3-642-33715-4"
}

@INPROCEEDINGS{Cityscapes,
  author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={The Cityscapes Dataset for Semantic Urban Scene Understanding}, 
  year={2016},
  volume={},
  number={},
  pages={3213-3223},
  keywords={Urban areas;Semantics;Visualization;Benchmark testing;Vehicles;Training;Complexity theory},
  doi={10.1109/CVPR.2016.350}
}


@INPROCEEDINGS{DispNet,
  author={Mayer, Nikolaus and Ilg, Eddy and Häusser, Philip and Fischer, Philipp and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation}, 
  year={2016},
  volume={},
  number={},
  pages={4040-4048},
  keywords={Estimation;Three-dimensional displays;Optical imaging;Training;Cameras;Optical sensors;Optical fiber networks},
  doi={10.1109/CVPR.2016.438}
}

@inproceedings{DIW,
  author = {Chen, Weifeng and Fu, Zhao and Yang, Dawei and Deng, Jia},
  title = {Single-image depth perception in the wild},
  year = {2016},
  isbn = {9781510838819},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  url= {https://websites.umich.edu/~wfchen/depth-in-the-wild/},
  abstract = {This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages = {730-738},
  numpages = {9},
  location = {Barcelona, Spain},
  series = {NIPS'16}
}

@INPROCEEDINGS{ReDWeb,
  author={Xian, Ke and Shen, Chunhua and Cao, Zhiguo and Lu, Hao and Xiao, Yang and Li, Ruibo and Luo, Zhenbo},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Monocular Relative Depth Perception with Web Stereo Data Supervision}, 
  year={2018},
  volume={},
  number={},
  pages={311-320},
  keywords={Training;Measurement;Task analysis;Semantics;Estimation;Image segmentation;Network architecture},
  doi={10.1109/CVPR.2018.00040}
}

@INPROCEEDINGS{Youtube3D,
  author={Chen, Weifeng and Qian, Shengyi and Deng, Jia},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Learning Single-Image Depth From Videos Using Quality Assessment Networks}, 
  year={2019},
  volume={},
  number={},
  pages={5597-5606},
  keywords={3D from Single Image;Datasets and Evaluation},
  doi={10.1109/CVPR.2019.00575}
}

##############################################################
#                        SSL
##############################################################

@InProceedings{Garg,
  author="Garg, Ravi
    and B.G., Vijay Kumar
    and Carneiro, Gustavo
    and Reid, Ian",
  editor="Leibe, Bastian
    and Matas, Jiri
    and Sebe, Nicu
    and Welling, Max",
  title="Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue",
  booktitle="Computer Vision -- ECCV 2016",
  year="2016",
  publisher="Springer International Publishing",
  address="Cham",
  pages="740--756",
  abstract="A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manually labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth prediction, without requiring a pre-training stage or annotated ground-truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photometric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset gives comparable performance to that of the state-of-the-art supervised methods for single view depth estimation.",
  isbn="978-3-319-46484-8"
}

@INPROCEEDINGS{MonoDepth,
  author={Godard, Clément and Aodha, Oisin Mac and Brostow, Gabriel J.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unsupervised Monocular Depth Estimation with Left-Right Consistency}, 
  year={2017},
  volume={},
  number={},
  pages={6602-6611},
  keywords={Estimation;Training;Image reconstruction;Cameras;Predictive models;Neural networks;Lighting},
  doi={10.1109/CVPR.2017.699}}

@INPROCEEDINGS{SfMLearner,
  author={Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unsupervised Learning of Depth and Ego-Motion from Video}, 
  year={2017},
  volume={},
  number={},
  pages={6612-6619},
  keywords={Cameras;Training;Pose estimation;Three-dimensional displays;Geometry;Pipelines},
  doi={10.1109/CVPR.2017.700},
  url={https://github.com/tinghuiz/SfMLearner},
}

@article{struct2depth,
  title={Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos},
  author={Vincent Casser and S{\"o}ren Pirk and Reza Mahjourian and Anelia Angelova},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.06152},
  url={https://api.semanticscholar.org/CorpusID:53437459}
}

@INPROCEEDINGS{MonoDepth2,
  author={Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Digging Into Self-Supervised Monocular Depth Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={3827-3837},
  keywords={Training;Estimation;Predictive models;Cameras;Image color analysis;Image reconstruction;Image matching},
  doi={10.1109/ICCV.2019.00393}}

@inproceedings{vid2depth,
      title={Unsupervised Learning of Depth and Ego-Motion
             from Monocular Video Using 3D Geometric
             Constraints},
      author={Mahjourian, Reza and Wicke, Martin and
              Angelova, Anelia},
      booktitle={CVPR},
      year={2018},
      url={https://sites.google.com/view/vid2depth},
}

@article{GCNDepth,
  title={Gcndepth: Self-supervised monocular depth estimation based on graph convolutional network},
  author={Masoumian, Armin and Rashwan, Hatem A and Abdulwahab, Saddam and Cristiano, Julian and Asif, M Salman and Puig, Domenec},
  journal={Neurocomputing},
  year={2022},
  publisher={Elsevier}
}

@InProceedings{FeatDepth,
  author="Shu, Chang
    and Yu, Kun
    and Duan, Zhixiang
    and Yang, Kuiyuan",
  editor="Vedaldi, Andrea
    and Bischof, Horst
    and Brox, Thomas
    and Frahm, Jan-Michael",
  title="Feature-Metric Loss for Self-supervised Learning of Depth and Egomotion",
  booktitle="Computer Vision -- ECCV 2020",
  year="2020",
  publisher="Springer International Publishing",
  address="Cham",
  pages="572--588",
  abstract="Photometric loss is widely used for self-supervised depth and egomotion estimation. However, the loss landscapes induced by photometric differences are often problematic for optimization, caused by plateau landscapes for pixels in textureless regions or multiple local minima for less discriminative pixels. In this work, feature-metric loss is proposed and defined on feature representation, where the feature representation is also learned in a self-supervised manner and regularized by both first-order and second-order derivatives to constrain the loss landscapes to form proper convergence basins. Comprehensive experiments and detailed analysis via visualization demonstrate the effectiveness of the proposed feature-metric loss. In particular, our method improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured by {\$}{\$}{\backslash}delta {\_}1{\$}{\$}$\delta$1for depth estimation, and significantly outperforms previous method for visual odometry.",
  isbn="978-3-030-58529-7"
}

##############################################################
#                        SOTA
##############################################################

@ARTICLE{MiDas,
  author={Ranftl, René and Lasinger, Katrin and Hafner, David and Schindler, Konrad and Koltun, Vladlen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer}, 
  year={2022},
  volume={44},
  number={3},
  pages={1623-1637},
  keywords={Training;Estimation;Three-dimensional displays;Cameras;Videos;Measurement;Motion pictures;Monocular depth estimation;single-image depth prediction;zero-shot cross-dataset transfer;multi-dataset training},
  doi={10.1109/TPAMI.2020.3019967},
  url={https://github.com/isl-org/MiDaS}
}

@article{PatchFusion,
  title={PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation}, 
  author={Zhenyu Li and Shariq Farooq Bhat and Peter Wonka},
  year={2023},
  eprint={2312.02284},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://zhyever.github.io/patchfusion/},
}

@InProceedings{Marigold,
    title={Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation},
    author={Bingxin Ke and Anton Obukhov and Shengyu Huang and Nando Metzger and Rodrigo Caye Daudt and Konrad Schindler},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2024},
    url={https://marigoldmonodepth.github.io/},
}

##############################################################
#                        surveys
##############################################################
@INPROCEEDINGS{correspondance,
  author={Scharstein, D. and Szeliski, R. and Zabih, R.},
  booktitle={Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)}, 
  title={A taxonomy and evaluation of dense two-frame stereo correspondence algorithms}, 
  year={2001},
  volume={},
  number={},
  pages={131-140},
  keywords={Taxonomy;Bismuth},
  doi={10.1109/SMBV.2001.988771}
}

@ARTICLE{stereo,
  author={Laga, Hamid and Jospin, Laurent Valentin and Boussaid, Farid and Bennamoun, Mohammed},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Survey on Deep Learning Techniques for Stereo-Based Depth Estimation}, 
  year={2022},
  volume={44},
  number={4},
  pages={1738-1764},
  keywords={Estimation;Videos;Deep learning;Three-dimensional displays;Australia;Training;Pipelines;CNN;deep learning;3D reconstruction;stereo matching;multi-view stereo;disparity estimation;feature leaning;feature matching},
  doi={10.1109/TPAMI.2020.3032602}
}

@ARTICLE{monocular2024,
  author={Arampatzakis, Vasileios and Pavlidis, George and Mitianoudis, Nikolaos and Papamarkos, Nikos},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Monocular Depth Estimation: A Thorough Review}, 
  year={2024},
  volume={46},
  number={4},
  pages={2396-2414},
  keywords={Estimation;Visualization;Observers;Three-dimensional displays;Pattern analysis;Visual perception;Real-time systems;Artificial intelligence;computer vision;deep learning;depth estimation;image processing;machine learning;monocular depth},
  doi={10.1109/TPAMI.2023.3330944}
}

@Article{monocular_SSL,
AUTHOR = {Masoumian, Armin and Rashwan, Hatem A. and Cristiano, Julián and Asif, M. Salman and Puig, Domenec},
TITLE = {Monocular Depth Estimation Using Deep Learning: A Review},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {14},
ARTICLE-NUMBER = {5353},
URL = {https://www.mdpi.com/1424-8220/22/14/5353},
PubMedID = {35891033},
ISSN = {1424-8220},
ABSTRACT = {In current decades, significant advancements in robotics engineering and autonomous vehicles have improved the requirement for precise depth measurements. Depth estimation (DE) is a traditional task in computer vision that can be appropriately predicted by applying numerous procedures. This task is vital in disparate applications such as augmented reality and target tracking. Conventional monocular DE (MDE) procedures are based on depth cues for depth prediction. Various deep learning techniques have demonstrated their potential applications in managing and supporting the traditional ill-posed problem. The principal purpose of this paper is to represent a state-of-the-art review of the current developments in MDE based on deep learning techniques. For this goal, this paper tries to highlight the critical points of the state-of-the-art works on MDE from disparate aspects. These aspects include input data shapes and training manners such as supervised, semi-supervised, and unsupervised learning approaches in combination with applying different datasets and evaluation indicators. At last, limitations regarding the accuracy of the DL-based MDE models, computational time requirements, real-time inference, transferability, input images shape and domain adaptation, and generalization are discussed to open new directions for future research.},
DOI = {10.3390/s22145353}
}

@article{evalStudy,
  title = {Depth prediction from 2D images: A taxonomy and an evaluation study},
  journal = {Image and Vision Computing},
  volume = {93},
  pages = {103825},
  year = {2020},
  issn = {0262-8856},
  doi = {https://doi.org/10.1016/j.imavis.2019.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0262885619304184},
  author = {Ambroise Moreau and Matei Mancas and Thierry Dutoit},
  keywords = {Depth prediction, Machine learning, Deep learning, Computer vision},
  abstract = {Among the various cues that help us understand and interact with our surroundings, depth is of particular importance. It allows us to move in space and grab objects to complete different tasks. Therefore, depth prediction has been an active research field for decades and many algorithms have been proposed to retrieve depth. Some imitate human vision and compute depth through triangulation on correspondences found between pixels or handcrafted features in different views of the same scene. Others rely on simple assumptions and semantic knowledge of the structure of the scene to get the depth information. Recently, numerous algorithms based on deep learning have emerged from the computer vision community. They implement the same principles as the non-deep learning methods and leverage the ability of deep neural networks of automatically learning important features that help to solve the task. By doing so, they produce new state-of-the-art results and show encouraging prospects. In this article, we propose a taxonomy of deep learning methods for depth prediction from 2D images. We retained the training strategy as the sorting criterion. Indeed, some methods are trained in a supervised manner which means depth labels are needed during training while others are trained in an unsupervised manner. In that case, the models learn to perform a different task such as view synthesis and depth is only a by-product of this learning. In addition to this taxonomy, we also evaluate nine models on two similar datasets without retraining. Our analysis showed that (i) most models are sensitive to sharp discontinuities created by shadows or colour contrasts and (ii) the post processing applied to the results before computing the commonly used metrics can change the model ranking. Moreover, we showed that most metrics agree with each other and are thus redundant.}
}

##############################################################
#                        utils
##############################################################

@InProceedings{UNet,
  author="Ronneberger, Olaf
    and Fischer, Philipp
    and Brox, Thomas",
  editor="Navab, Nassir
    and Hornegger, Joachim
    and Wells, William M.
    and Frangi, Alejandro F.",
  title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
  booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
  year="2015",
  publisher="Springer International Publishing",
  address="Cham",
  pages="234--241",
  abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
  isbn="978-3-319-24574-4"
}

@ARTICLE{SSIM,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}
}

@INPROCEEDINGS{FlowNet,
  author={Dosovitskiy, Alexey and Fischer, Philipp and Ilg, Eddy and Häusser, Philip and Hazirbas, Caner and Golkov, Vladimir and Smagt, Patrick van der and Cremers, Daniel and Brox, Thomas},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={FlowNet: Learning Optical Flow with Convolutional Networks}, 
  year={2015},
  volume={},
  number={},
  pages={2758-2766},
  keywords={Optical imaging;Computer architecture;Image resolution;Correlation;Optical fiber networks;Neural networks;Optical computing},
  doi={10.1109/ICCV.2015.316}
}

@inproceedings{STN,
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  title = {Spatial transformer networks},
  year = {2015},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
  pages = {2017-2025},
  numpages = {9},
  location = {Montreal, Canada},
  series = {NIPS'15}
}

@inproceedings{pareto,
  author = {Sener, Ozan and Koltun, Vladlen},
  title = {Multi-task learning as multi-objective optimization},
  year = {2018},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages = {525-536},
  numpages = {12},
  location = {Montr\'{e}al, Canada},
  series = {NIPS'18}
}

@article{ViT,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@INPROCEEDINGS{swin,
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
  year={2021},
  volume={},
  number={},
  pages={9992-10002},
  keywords={Image segmentation;Computer vision;Visualization;Computational modeling;Semantics;Object detection;Computer architecture;Representation learning;Detection and localization in 2D and 3D;Recognition and classification;Segmentation;grouping and shape},
  doi={10.1109/ICCV48922.2021.00986}
}

@INPROCEEDINGS{StableDiffusionV2,
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={10674-10685},
  keywords={Training;Visualization;Image synthesis;Computational modeling;Noise reduction;Superresolution;Process control;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01042}
}

##############################################################
#                        classification
##############################################################

@inproceedings{AlexNet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{VGG,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2014},
  volume={abs/1409.1556},
  url={https://api.semanticscholar.org/CorpusID:14124313}
}

@INPROCEEDINGS{ResNet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}
}

@INPROCEEDINGS{Inception,
  author={Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Going deeper with convolutions}, 
  year={2015},
  volume={},
  number={},
  pages={1-9},
  keywords={Computer architecture;Convolutional codes;Sparse matrices;Neural networks;Visualization;Object detection;Computer vision},
  doi={10.1109/CVPR.2015.7298594}
}