@misc{example-citation,
  author = {Tullio Facchinetti},
  title = {How to write a thesis in LaTeX},
  note = {\url{http://robot.unipv.it/toolleeo}, Last Access 2021.06.21},
  year = {2021}
}

##############################################################
#                        BOOKS
##############################################################

@book{multiview,
    place={Cambridge},
    edition={2},
    title={Multiple View Geometry in Computer Vision},
    publisher={Cambridge University Press},
    author={Hartley, Richard and Zisserman, Andrew},
    year={2004}
}

@book{computer_vision,
    edition={2},
    title="Computer Vision",
    publisher="Springer Cham",
    author="Richard Szeliski",
    year={2011}
}

##############################################################
#                        CLASSIC
##############################################################

@INPROCEEDINGS{IM2CAD,
  author={Izadinia, Hamid and Shan, Qi and Seitz, Steven M.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={IM2CAD}, 
  year={2017},
  volume={},
  number={},
  pages={2422-2431},
  keywords={Solid modeling;Three-dimensional displays;Computational modeling;Databases;Estimation;Image reconstruction},
  doi={10.1109/CVPR.2017.260}
}

@article{autopopup1,
author = {Hoiem, Derek and Efros, Alexei A. and Hebert, Martial},
title = {Automatic photo pop-up},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {3},
issn = {0730-0301},
url = {https://doi.org/10.1145/1073204.1073232},
doi = {10.1145/1073204.1073232},
abstract = {This paper presents a fully automatic method for creating a 3D model from a single photograph. The model is made up of several texture-mapped planar billboards and has the complexity of a typical children's pop-up book illustration. Our main insight is that instead of attempting to recover precise geometry, we statistically model geometric classes defined by their orientations in the scene. Our algorithm labels regions of the input image into coarse categories: "ground", "sky", and "vertical". These labels are then used to "cut and fold" the image into a pop-up model using a set of simple assumptions. Because of the inherent ambiguity of the problem and the statistical nature of the approach, the algorithm is not expected to work on every image. However. it performs surprisingly well for a wide range of scenes taken from a typical person's photo album.},
journal = {ACM Trans. Graph.},
month = {jul},
pages = {577-584},
numpages = {8},
keywords = {single-view reconstruction, machine learning, image-based rendering, image segmentation}
}

@INPROCEEDINGS{autopopup2,
  author={Hoiem, D. and Efros, A.A. and Hebert, M.},
  booktitle={Tenth IEEE International Conference on Computer Vision (ICCV'05) Volume 1}, 
  title={Geometric context from a single image}, 
  year={2005},
  volume={1},
  number={},
  pages={654-661 Vol. 1},
  keywords={Layout;Object detection;Computer vision;Surface reconstruction;Cameras;Application software;Image reconstruction;Humans;Solid modeling;Robustness},
  doi={10.1109/ICCV.2005.107}}

@INPROCEEDINGS{autopopup3,
  author={Hoiem, D. and Efros, A.A. and Hebert, M.},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)}, 
  title={Putting Objects in Perspective}, 
  year={2006},
  volume={2},
  number={},
  pages={2137-2144},
  keywords={Layout;Object detection;Cameras;Roads;Context modeling;Geometry;Detectors;Computer vision;Robot kinematics;Solid modeling},
  doi={10.1109/CVPR.2006.232}}

@INPROCEEDINGS{autopopup4,
  author={Hoiem, Derek and Stein, Andrew N. and Efros, Alexei A. and Hebert, Martial},
  booktitle={2007 IEEE 11th International Conference on Computer Vision}, 
  title={Recovering Occlusion Boundaries from a Single Image}, 
  year={2007},
  volume={},
  number={},
  pages={1-8},
  keywords={Layout;Computer vision;Humans;Vegetation mapping;Robot vision systems;Navigation;Image segmentation;Displays;Image processing;Biomedical imaging},
  doi={10.1109/ICCV.2007.4408985}}

@Article{autopopup5,
author={Hoiem, Derek
and Efros, Alexei A.
and Hebert, Martial},
title={Recovering Surface Layout from an Image},
journal={International Journal of Computer Vision},
year={2007},
month={Oct},
day={01},
volume={75},
number={1},
pages={151-172},
abstract={Humans have an amazing ability to instantly grasp the overall 3D structure of a scene---ground orientation, relative positions of major landmarks, etc.---even from a single image. This ability is completely missing in most popular recognition algorithms, which pretend that the world is flat and/or view it through a patch-sized peephole. Yet it seems very likely that having a grasp of this ``surface layout'' of a scene should be of great assistance for many tasks, including recognition, navigation, and novel view synthesis.},
issn={1573-1405},
doi={10.1007/s11263-006-0031-y},
url={https://doi.org/10.1007/s11263-006-0031-y}
}


@INPROCEEDINGS{autopopup6,
  author={Hoiem, Derek and Efros, Alexei A. and Hebert, Martial},
  booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Closing the loop in scene interpretation}, 
  year={2008},
  volume={},
  number={},
  pages={1-8},
  keywords={Layout;Image analysis;Surface reconstruction;Feedforward systems;Object detection;Inference algorithms;Robot kinematics;Cameras;Object recognition;Reflectivity},
  doi={10.1109/CVPR.2008.4587587}}

@InProceedings{VideoCompass,
author="Ko{\v{s}}eck{\'a}, Jana
and Zhang, Wei",
editor="Heyden, Anders
and Sparr, Gunnar
and Nielsen, Mads
and Johansen, Peter",
title="Video Compass",
booktitle="Computer Vision --- ECCV 2002",
year="2002",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="476--490",
abstract="In this paper we describe a flexible approach for determining the relative orientation of the camera with respect to the scene. The main premise of the approach is the fact that in man-made environments, the majority of lines is aligned with the principal orthogonal directions of the world coordinate frame. We exploit this observation towards efficient detection and estimation of vanishing points, which provide strong constraints on camera parameters and relative orientation of the camera with respect to the scene.",
isbn="978-3-540-47979-6"
}


##############################################################
#                        SL
##############################################################
@INPROCEEDINGS{360MonoDepth,
  author={Rey-Area, Manuel and Yuan, Mingze and Richardt, Christian},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={360MonoDepth: High-Resolution 360° Monocular Depth Estimation}, 
  year={2022},
  volume={},
  number={},
  pages={3752-3762},
  keywords={Computer vision;Image resolution;Three-dimensional displays;Codes;Estimation;Graphics processing units;Virtual reality;3D from single images},
  doi={10.1109/CVPR52688.2022.00374}}


@INPROCEEDINGS{BoostingDepth,
  author={Miangoleh, S. Mahdi H. and Dille, Sebastian and Mai, Long and Paris, Sylvain and Aksoy, Yağız},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Boosting Monocular Depth Estimation Models to High-Resolution via Content-Adaptive Multi-Resolution Merging}, 
  year={2021},
  volume={},
  number={},
  pages={9680-9689},
  keywords={Location awareness;Image segmentation;Computer vision;Image resolution;Merging;Neural networks;Estimation},
  doi={10.1109/CVPR46437.2021.00956},
}

@ARTICLE{depth_as_classification,
  author={Cao, Yuanzhouhan and Wu, Zifeng and Shen, Chunhua},
  journal={IEEE Transactions on Circuits and Systems for Video Technology}, 
  title={Estimating Depth From Monocular Images as Classification Using Deep Fully Convolutional Residual Networks}, 
  year={2018},
  volume={28},
  number={11},
  pages={3174-3182},
  keywords={Estimation;Training;Semantics;Network architecture;Predictive models;Neural networks;Probability distribution;Classification;deep residual networks;depth estimation},
  doi={10.1109/TCSVT.2017.2740321}
}

@INPROCEEDINGS{ordinal_regression,
  author={Fu, Huan and Gong, Mingming and Wang, Chaohui and Batmanghelich, Kayhan and Tao, Dacheng},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Deep Ordinal Regression Network for Monocular Depth Estimation}, 
  year={2018},
  volume={},
  number={},
  pages={2002-2011},
  keywords={Estimation;Feature extraction;Training;Spatial resolution;Kernel;Three-dimensional displays;Two dimensional displays},
  doi={10.1109/CVPR.2018.00214}
}

@INPROCEEDINGS{AdaBins,
  author={Farooq Bhat, Shariq and Alhashim, Ibraheem and Wonka, Peter},
  booktitle={2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={AdaBins: Depth Estimation Using Adaptive Bins}, 
  year={2021},
  volume={},
  number={},
  pages={4008-4017},
  keywords={Measurement;Image segmentation;Three-dimensional displays;Image resolution;Estimation;Computer architecture;Transformers},
  doi={10.1109/CVPR46437.2021.00400},
  url={https://github.com/isl-org/ZoeDepth},
}

@InProceedings{LocalBins,
    author="Bhat, Shariq Farooq
        and Alhashim, Ibraheem
        and Wonka, Peter",
    editor="Avidan, Shai
        and Brostow, Gabriel
        and Ciss{\'e}, Moustapha
        and Farinella, Giovanni Maria
        and Hassner, Tal",
    title="LocalBins: Improving Depth Estimation by Learning Local Distributions",
    booktitle="Computer Vision -- ECCV 2022",
    year="2022",
    publisher="Springer Nature Switzerland",
    address="Cham",
    pages="480--496",
    abstract="We propose a novelarchitecture for depth estimation from a single image. The architecture itself is based on the popular encoder-decoder architecture that is frequently used as a starting point for all dense regression tasks. We build on AdaBins which estimates a global distribution of depth values for the input image and evolve the architecture in two ways. First, instead of predicting global depth distributions, we predict depth distributions of local neighborhoods at every pixel. Second, instead of predicting depth distributions only towards the end of the decoder, we involve all layers of the decoder. We call this new architecture LocalBins. Our results demonstrate a clear improvement over the state-of-the-art in all metrics on the NYU-Depth V2 dataset. Code and pretrained models will be made publicly available (https://github.com/shariqfarooq123/LocalBins).",
    isbn="978-3-031-19769-7",
    url={https://github.com/shariqfarooq123/LocalBins},
}


@misc{ZoeDepth,
  doi = {10.48550/ARXIV.2302.12288},
  url = {https://arxiv.org/abs/2302.12288},
  author = {Bhat, Shariq Farooq and Birkl, Reiner and Wofk, Diana and Wonka, Peter and Müller, Matthias},
  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title = {ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth},
  publisher = {arXiv},
  year = {2023},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@inproceedings{Eigen,
    author = {Eigen, David and Puhrsch, Christian and Fergus, Rob},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K.Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Depth Map Prediction from a Single Image using a Multi-Scale Deep Network},
    url = {https://proceedings.neurips.cc/paper_files/paper/2014/file/7bccfde7714a1ebadf06c5f4cea752c1-Paper.pdf},
    volume = {27},
    year = {2014}
}

@INPROCEEDINGS{Eigen2,
  author={Eigen, David and Fergus, Rob},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Predicting Depth, Surface Normals and Semantic Labels with a Common Multi-scale Convolutional Architecture}, 
  year={2015},
  volume={},
  number={},
  pages={2650-2658},
  keywords={Semantics;Estimation;Labeling;Image segmentation;Adaptation models;Spatial resolution},
  doi={10.1109/ICCV.2015.304}
}

@INPROCEEDINGS{Laina,
  author={Laina, Iro and Rupprecht, Christian and Belagiannis, Vasileios and Tombari, Federico and Navab, Nassir},
  booktitle={2016 Fourth International Conference on 3D Vision (3DV)}, 
  title={Deeper Depth Prediction with Fully Convolutional Residual Networks}, 
  year={2016},
  volume={},
  number={},
  pages={239-248},
  keywords={Estimation;Image resolution;Three-dimensional displays;Computer architecture;Computer vision;Semantics;Optimization;Depth prediction;CNN},
  doi={10.1109/3DV.2016.32}
}

@INPROCEEDINGS{Zoran,
  author={Zoran, Daniel and Isola, Phillip and Krishnan, Dilip and Freeman, William T.},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={Learning Ordinal Relationships for Mid-Level Vision}, 
  year={2015},
  volume={},
  number={},
  pages={388-396},
  keywords={Measurement;Estimation;Image edge detection;Image decomposition;Context;Image segmentation;Marine vehicles},
  doi={10.1109/ICCV.2015.52},
  url={ https://dilipkay.wordpress.com/ordinals/}
}

@INPROCEEDINGS{denseViT,
  author={Ranftl, René and Bochkovskiy, Alexey and Koltun, Vladlen},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Vision Transformers for Dense Prediction}, 
  year={2021},
  volume={},
  number={},
  pages={12159-12168},
  keywords={Computer vision;Image resolution;Semantics;Neural networks;Estimation;Training data;Computer architecture;Machine learning architectures and formulations;3D from a single image and shape-from-x;Segmentation;grouping and shape},
  doi={10.1109/ICCV48922.2021.01196}
}

##############################################################
#                        DATASETS
##############################################################

@INPROCEEDINGS{KITTI,
  author={Geiger, Andreas and Lenz, Philip and Urtasun, Raquel},
  booktitle={2012 IEEE Conference on Computer Vision and Pattern Recognition}, 
  title={Are we ready for autonomous driving? The KITTI vision benchmark suite}, 
  year={2012},
  volume={},
  number={},
  pages={3354-3361},
  keywords={Benchmark testing;Cameras;Optical imaging;Visualization;Optical sensors;Measurement},
  doi={10.1109/CVPR.2012.6248074}
}

@InProceedings{NYUv2,
  author="Silberman, Nathan
    and Hoiem, Derek
    and Kohli, Pushmeet
    and Fergus, Rob",
  editor="Fitzgibbon, Andrew
    and Lazebnik, Svetlana
    and Perona, Pietro
    and Sato, Yoichi
    and Schmid, Cordelia",
  title="Indoor Segmentation and Support Inference from RGBD Images",
  booktitle="Computer Vision -- ECCV 2012",
  year="2012",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="746--760",
  abstract="We present an approach to interpret the major surfaces, objects, and support relations of an indoor scene from an RGBD image. Most existing work ignores physical interactions or is applied only to tidy rooms and hallways. Our goal is to parse typical, often messy, indoor scenes into floor, walls, supporting surfaces, and object regions, and to recover support relationships. One of our main interests is to better understand how 3D cues can best inform a structured 3D interpretation. We also contribute a novel integer programming formulation to infer physical support relations. We offer a new dataset of 1449 RGBD images, capturing 464 diverse indoor scenes, with detailed annotations. Our experiments demonstrate our ability to infer support relations in complex scenes and verify that our 3D scene cues and inferred support lead to better object segmentation.",
  isbn="978-3-642-33715-4"
}

@INPROCEEDINGS{Cityscapes,
  author={Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={The Cityscapes Dataset for Semantic Urban Scene Understanding}, 
  year={2016},
  volume={},
  number={},
  pages={3213-3223},
  keywords={Urban areas;Semantics;Visualization;Benchmark testing;Vehicles;Training;Complexity theory},
  doi={10.1109/CVPR.2016.350}
}


@INPROCEEDINGS{DispNet,
  author={Mayer, Nikolaus and Ilg, Eddy and Häusser, Philip and Fischer, Philipp and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation}, 
  year={2016},
  volume={},
  number={},
  pages={4040-4048},
  keywords={Estimation;Three-dimensional displays;Optical imaging;Training;Cameras;Optical sensors;Optical fiber networks},
  doi={10.1109/CVPR.2016.438}
}

@inproceedings{DIW,
  author = {Chen, Weifeng and Fu, Zhao and Yang, Dawei and Deng, Jia},
  title = {Single-image depth perception in the wild},
  year = {2016},
  isbn = {9781510838819},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  url= {https://websites.umich.edu/~wfchen/depth-in-the-wild/},
  abstract = {This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild.},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages = {730-738},
  numpages = {9},
  location = {Barcelona, Spain},
  series = {NIPS'16}
}

@INPROCEEDINGS{ReDWeb,
  author={Xian, Ke and Shen, Chunhua and Cao, Zhiguo and Lu, Hao and Xiao, Yang and Li, Ruibo and Luo, Zhenbo},
  booktitle={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition}, 
  title={Monocular Relative Depth Perception with Web Stereo Data Supervision}, 
  year={2018},
  volume={},
  number={},
  pages={311-320},
  keywords={Training;Measurement;Task analysis;Semantics;Estimation;Image segmentation;Network architecture},
  doi={10.1109/CVPR.2018.00040}
}

@INPROCEEDINGS{Youtube3D,
  author={Chen, Weifeng and Qian, Shengyi and Deng, Jia},
  booktitle={2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Learning Single-Image Depth From Videos Using Quality Assessment Networks}, 
  year={2019},
  volume={},
  number={},
  pages={5597-5606},
  keywords={3D from Single Image;Datasets and Evaluation},
  doi={10.1109/CVPR.2019.00575}
}

##############################################################
#                        SSL
##############################################################

@InProceedings{Garg,
  author="Garg, Ravi
    and B.G., Vijay Kumar
    and Carneiro, Gustavo
    and Reid, Ian",
  editor="Leibe, Bastian
    and Matas, Jiri
    and Sebe, Nicu
    and Welling, Max",
  title="Unsupervised CNN for Single View Depth Estimation: Geometry to the Rescue",
  booktitle="Computer Vision -- ECCV 2016",
  year="2016",
  publisher="Springer International Publishing",
  address="Cham",
  pages="740--756",
  abstract="A significant weakness of most current deep Convolutional Neural Networks is the need to train them using vast amounts of manually labelled data. In this work we propose a unsupervised framework to learn a deep convolutional neural network for single view depth prediction, without requiring a pre-training stage or annotated ground-truth depths. We achieve this by training the network in a manner analogous to an autoencoder. At training time we consider a pair of images, source and target, with small, known camera motion between the two such as a stereo pair. We train the convolutional encoder for the task of predicting the depth map for the source image. To do so, we explicitly generate an inverse warp of the target image using the predicted depth and known inter-view displacement, to reconstruct the source image; the photometric error in the reconstruction is the reconstruction loss for the encoder. The acquisition of this training data is considerably simpler than for equivalent systems, requiring no manual annotation, nor calibration of depth sensor to camera. We show that our network trained on less than half of the KITTI dataset gives comparable performance to that of the state-of-the-art supervised methods for single view depth estimation.",
  isbn="978-3-319-46484-8"
}

@INPROCEEDINGS{MonoDepth,
  author={Godard, Clément and Aodha, Oisin Mac and Brostow, Gabriel J.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unsupervised Monocular Depth Estimation with Left-Right Consistency}, 
  year={2017},
  volume={},
  number={},
  pages={6602-6611},
  keywords={Estimation;Training;Image reconstruction;Cameras;Predictive models;Neural networks;Lighting},
  doi={10.1109/CVPR.2017.699}}

@INPROCEEDINGS{SfMLearner,
  author={Zhou, Tinghui and Brown, Matthew and Snavely, Noah and Lowe, David G.},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Unsupervised Learning of Depth and Ego-Motion from Video}, 
  year={2017},
  volume={},
  number={},
  pages={6612-6619},
  keywords={Cameras;Training;Pose estimation;Three-dimensional displays;Geometry;Pipelines},
  doi={10.1109/CVPR.2017.700},
  url={https://github.com/tinghuiz/SfMLearner},
}

@article{struct2depth,
  title={Depth Prediction Without the Sensors: Leveraging Structure for Unsupervised Learning from Monocular Videos},
  author={Vincent Casser and S{\"o}ren Pirk and Reza Mahjourian and Anelia Angelova},
  journal={ArXiv},
  year={2018},
  volume={abs/1811.06152},
  url={https://api.semanticscholar.org/CorpusID:53437459}
}

@INPROCEEDINGS{MonoDepth2,
  author={Godard, Clement and Aodha, Oisin Mac and Firman, Michael and Brostow, Gabriel},
  booktitle={2019 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Digging Into Self-Supervised Monocular Depth Estimation}, 
  year={2019},
  volume={},
  number={},
  pages={3827-3837},
  keywords={Training;Estimation;Predictive models;Cameras;Image color analysis;Image reconstruction;Image matching},
  doi={10.1109/ICCV.2019.00393}}

@inproceedings{vid2depth,
      title={Unsupervised Learning of Depth and Ego-Motion
             from Monocular Video Using 3D Geometric
             Constraints},
      author={Mahjourian, Reza and Wicke, Martin and
              Angelova, Anelia},
      booktitle={CVPR},
      year={2018},
      url={https://sites.google.com/view/vid2depth},
}

@article{GCNDepth,
  title={Gcndepth: Self-supervised monocular depth estimation based on graph convolutional network},
  author={Masoumian, Armin and Rashwan, Hatem A and Abdulwahab, Saddam and Cristiano, Julian and Asif, M Salman and Puig, Domenec},
  journal={Neurocomputing},
  year={2022},
  publisher={Elsevier}
}

@InProceedings{FeatDepth,
  author="Shu, Chang
    and Yu, Kun
    and Duan, Zhixiang
    and Yang, Kuiyuan",
  editor="Vedaldi, Andrea
    and Bischof, Horst
    and Brox, Thomas
    and Frahm, Jan-Michael",
  title="Feature-Metric Loss for Self-supervised Learning of Depth and Egomotion",
  booktitle="Computer Vision -- ECCV 2020",
  year="2020",
  publisher="Springer International Publishing",
  address="Cham",
  pages="572--588",
  abstract="Photometric loss is widely used for self-supervised depth and egomotion estimation. However, the loss landscapes induced by photometric differences are often problematic for optimization, caused by plateau landscapes for pixels in textureless regions or multiple local minima for less discriminative pixels. In this work, feature-metric loss is proposed and defined on feature representation, where the feature representation is also learned in a self-supervised manner and regularized by both first-order and second-order derivatives to constrain the loss landscapes to form proper convergence basins. Comprehensive experiments and detailed analysis via visualization demonstrate the effectiveness of the proposed feature-metric loss. In particular, our method improves state-of-the-art methods on KITTI from 0.885 to 0.925 measured by {\$}{\$}{\backslash}delta {\_}1{\$}{\$}$\delta$1for depth estimation, and significantly outperforms previous method for visual odometry.",
  isbn="978-3-030-58529-7"
}

##############################################################
#                        SOTA
##############################################################

@ARTICLE{MiDas,
  author={Ranftl, René and Lasinger, Katrin and Hafner, David and Schindler, Konrad and Koltun, Vladlen},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer}, 
  year={2022},
  volume={44},
  number={3},
  pages={1623-1637},
  keywords={Training;Estimation;Three-dimensional displays;Cameras;Videos;Measurement;Motion pictures;Monocular depth estimation;single-image depth prediction;zero-shot cross-dataset transfer;multi-dataset training},
  doi={10.1109/TPAMI.2020.3019967},
  url={https://github.com/isl-org/MiDaS}
}

@article{PatchFusion,
  title={PatchFusion: An End-to-End Tile-Based Framework for High-Resolution Monocular Metric Depth Estimation}, 
  author={Zhenyu Li and Shariq Farooq Bhat and Peter Wonka},
  year={2023},
  eprint={2312.02284},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://zhyever.github.io/patchfusion/},
}

@InProceedings{Marigold,
    title={Repurposing Diffusion-Based Image Generators for Monocular Depth Estimation},
    author={Bingxin Ke and Anton Obukhov and Shengyu Huang and Nando Metzger and Rodrigo Caye Daudt and Konrad Schindler},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year={2024},
    url={https://marigoldmonodepth.github.io/},
}

##############################################################
#                        surveys
##############################################################
@INPROCEEDINGS{correspondance,
  author={Scharstein, D. and Szeliski, R. and Zabih, R.},
  booktitle={Proceedings IEEE Workshop on Stereo and Multi-Baseline Vision (SMBV 2001)}, 
  title={A taxonomy and evaluation of dense two-frame stereo correspondence algorithms}, 
  year={2001},
  volume={},
  number={},
  pages={131-140},
  keywords={Taxonomy;Bismuth},
  doi={10.1109/SMBV.2001.988771}
}

@ARTICLE{stereo,
  author={Laga, Hamid and Jospin, Laurent Valentin and Boussaid, Farid and Bennamoun, Mohammed},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={A Survey on Deep Learning Techniques for Stereo-Based Depth Estimation}, 
  year={2022},
  volume={44},
  number={4},
  pages={1738-1764},
  keywords={Estimation;Videos;Deep learning;Three-dimensional displays;Australia;Training;Pipelines;CNN;deep learning;3D reconstruction;stereo matching;multi-view stereo;disparity estimation;feature leaning;feature matching},
  doi={10.1109/TPAMI.2020.3032602}
}

@ARTICLE{monocular2024,
  author={Arampatzakis, Vasileios and Pavlidis, George and Mitianoudis, Nikolaos and Papamarkos, Nikos},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Monocular Depth Estimation: A Thorough Review}, 
  year={2024},
  volume={46},
  number={4},
  pages={2396-2414},
  keywords={Estimation;Visualization;Observers;Three-dimensional displays;Pattern analysis;Visual perception;Real-time systems;Artificial intelligence;computer vision;deep learning;depth estimation;image processing;machine learning;monocular depth},
  doi={10.1109/TPAMI.2023.3330944}
}

@Article{monocular_SSL,
AUTHOR = {Masoumian, Armin and Rashwan, Hatem A. and Cristiano, Julián and Asif, M. Salman and Puig, Domenec},
TITLE = {Monocular Depth Estimation Using Deep Learning: A Review},
JOURNAL = {Sensors},
VOLUME = {22},
YEAR = {2022},
NUMBER = {14},
ARTICLE-NUMBER = {5353},
URL = {https://www.mdpi.com/1424-8220/22/14/5353},
PubMedID = {35891033},
ISSN = {1424-8220},
ABSTRACT = {In current decades, significant advancements in robotics engineering and autonomous vehicles have improved the requirement for precise depth measurements. Depth estimation (DE) is a traditional task in computer vision that can be appropriately predicted by applying numerous procedures. This task is vital in disparate applications such as augmented reality and target tracking. Conventional monocular DE (MDE) procedures are based on depth cues for depth prediction. Various deep learning techniques have demonstrated their potential applications in managing and supporting the traditional ill-posed problem. The principal purpose of this paper is to represent a state-of-the-art review of the current developments in MDE based on deep learning techniques. For this goal, this paper tries to highlight the critical points of the state-of-the-art works on MDE from disparate aspects. These aspects include input data shapes and training manners such as supervised, semi-supervised, and unsupervised learning approaches in combination with applying different datasets and evaluation indicators. At last, limitations regarding the accuracy of the DL-based MDE models, computational time requirements, real-time inference, transferability, input images shape and domain adaptation, and generalization are discussed to open new directions for future research.},
DOI = {10.3390/s22145353}
}

@article{evalStudy,
  title = {Depth prediction from 2D images: A taxonomy and an evaluation study},
  journal = {Image and Vision Computing},
  volume = {93},
  pages = {103825},
  year = {2020},
  issn = {0262-8856},
  doi = {https://doi.org/10.1016/j.imavis.2019.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0262885619304184},
  author = {Ambroise Moreau and Matei Mancas and Thierry Dutoit},
  keywords = {Depth prediction, Machine learning, Deep learning, Computer vision},
  abstract = {Among the various cues that help us understand and interact with our surroundings, depth is of particular importance. It allows us to move in space and grab objects to complete different tasks. Therefore, depth prediction has been an active research field for decades and many algorithms have been proposed to retrieve depth. Some imitate human vision and compute depth through triangulation on correspondences found between pixels or handcrafted features in different views of the same scene. Others rely on simple assumptions and semantic knowledge of the structure of the scene to get the depth information. Recently, numerous algorithms based on deep learning have emerged from the computer vision community. They implement the same principles as the non-deep learning methods and leverage the ability of deep neural networks of automatically learning important features that help to solve the task. By doing so, they produce new state-of-the-art results and show encouraging prospects. In this article, we propose a taxonomy of deep learning methods for depth prediction from 2D images. We retained the training strategy as the sorting criterion. Indeed, some methods are trained in a supervised manner which means depth labels are needed during training while others are trained in an unsupervised manner. In that case, the models learn to perform a different task such as view synthesis and depth is only a by-product of this learning. In addition to this taxonomy, we also evaluate nine models on two similar datasets without retraining. Our analysis showed that (i) most models are sensitive to sharp discontinuities created by shadows or colour contrasts and (ii) the post processing applied to the results before computing the commonly used metrics can change the model ranking. Moreover, we showed that most metrics agree with each other and are thus redundant.}
}

##############################################################
#                        utils
##############################################################

@InProceedings{UNet,
  author="Ronneberger, Olaf
    and Fischer, Philipp
    and Brox, Thomas",
  editor="Navab, Nassir
    and Hornegger, Joachim
    and Wells, William M.
    and Frangi, Alejandro F.",
  title="U-Net: Convolutional Networks for Biomedical Image Segmentation",
  booktitle="Medical Image Computing and Computer-Assisted Intervention -- MICCAI 2015",
  year="2015",
  publisher="Springer International Publishing",
  address="Cham",
  pages="234--241",
  abstract="There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net.",
  isbn="978-3-319-24574-4"
}

@ARTICLE{SSIM,
  author={Zhou Wang and Bovik, A.C. and Sheikh, H.R. and Simoncelli, E.P.},
  journal={IEEE Transactions on Image Processing}, 
  title={Image quality assessment: from error visibility to structural similarity}, 
  year={2004},
  volume={13},
  number={4},
  pages={600-612},
  keywords={Image quality;Humans;Transform coding;Visual system;Visual perception;Data mining;Layout;Quality assessment;Degradation;Indexes},
  doi={10.1109/TIP.2003.819861}
}

@INPROCEEDINGS{FlowNet,
  author={Dosovitskiy, Alexey and Fischer, Philipp and Ilg, Eddy and Häusser, Philip and Hazirbas, Caner and Golkov, Vladimir and Smagt, Patrick van der and Cremers, Daniel and Brox, Thomas},
  booktitle={2015 IEEE International Conference on Computer Vision (ICCV)}, 
  title={FlowNet: Learning Optical Flow with Convolutional Networks}, 
  year={2015},
  volume={},
  number={},
  pages={2758-2766},
  keywords={Optical imaging;Computer architecture;Image resolution;Correlation;Optical fiber networks;Neural networks;Optical computing},
  doi={10.1109/ICCV.2015.316}
}

@inproceedings{STN,
  author = {Jaderberg, Max and Simonyan, Karen and Zisserman, Andrew and Kavukcuoglu, Koray},
  title = {Spatial transformer networks},
  year = {2015},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  abstract = {Convolutional Neural Networks define an exceptionally powerful class of models, but are still limited by the lack of ability to be spatially invariant to the input data in a computationally and parameter efficient manner. In this work we introduce a new learnable module, the Spatial Transformer, which explicitly allows the spatial manipulation of data within the network. This differentiable module can be inserted into existing convolutional architectures, giving neural networks the ability to actively spatially transform feature maps, conditional on the feature map itself, without any extra training supervision or modification to the optimisation process. We show that the use of spatial transformers results in models which learn invariance to translation, scale, rotation and more generic warping, resulting in state-of-the-art performance on several benchmarks, and for a number of classes of transformations.},
  booktitle = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
  pages = {2017-2025},
  numpages = {9},
  location = {Montreal, Canada},
  series = {NIPS'15}
}

@inproceedings{pareto,
  author = {Sener, Ozan and Koltun, Vladlen},
  title = {Multi-task learning as multi-objective optimization},
  year = {2018},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  abstract = {In multi-task learning, multiple tasks are solved jointly, sharing inductive bias between them. Multi-task learning is inherently a multi-objective problem because different tasks may conflict, necessitating a trade-off. A common compromise is to optimize a proxy objective that minimizes a weighted linear combination of per-task losses. However, this workaround is only valid when the tasks do not compete, which is rarely the case. In this paper, we explicitly cast multi-task learning as multi-objective optimization, with the overall objective of finding a Pareto optimal solution. To this end, we use algorithms developed in the gradient-based multi-objective optimization literature. These algorithms are not directly applicable to large-scale learning problems since they scale poorly with the dimensionality of the gradients and the number of tasks. We therefore propose an upper bound for the multi-objective loss and show that it can be optimized efficiently. We further prove that optimizing this upper bound yields a Pareto optimal solution under realistic assumptions. We apply our method to a variety of multi-task deep learning problems including digit classification, scene understanding (joint semantic segmentation, instance segmentation, and depth estimation), and multi-label classification. Our method produces higher-performing models than recent multi-task learning formulations or per-task training.},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  pages = {525-536},
  numpages = {12},
  location = {Montr\'{e}al, Canada},
  series = {NIPS'18}
}

@article{ViT,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@INPROCEEDINGS{swin,
  author={Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
  booktitle={2021 IEEE/CVF International Conference on Computer Vision (ICCV)}, 
  title={Swin Transformer: Hierarchical Vision Transformer using Shifted Windows}, 
  year={2021},
  volume={},
  number={},
  pages={9992-10002},
  keywords={Image segmentation;Computer vision;Visualization;Computational modeling;Semantics;Object detection;Computer architecture;Representation learning;Detection and localization in 2D and 3D;Recognition and classification;Segmentation;grouping and shape},
  doi={10.1109/ICCV48922.2021.00986}
}

@INPROCEEDINGS{StableDiffusionV2,
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
  booktitle={2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={High-Resolution Image Synthesis with Latent Diffusion Models}, 
  year={2022},
  volume={},
  number={},
  pages={10674-10685},
  keywords={Training;Visualization;Image synthesis;Computational modeling;Noise reduction;Superresolution;Process control;Image and video synthesis and generation},
  doi={10.1109/CVPR52688.2022.01042}
}

##############################################################
#                        classification
##############################################################

@inproceedings{AlexNet,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@article{VGG,
  title={Very Deep Convolutional Networks for Large-Scale Image Recognition},
  author={Karen Simonyan and Andrew Zisserman},
  journal={CoRR},
  year={2014},
  volume={abs/1409.1556},
  url={https://api.semanticscholar.org/CorpusID:14124313}
}

@INPROCEEDINGS{ResNet,
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Deep Residual Learning for Image Recognition}, 
  year={2016},
  volume={},
  number={},
  pages={770-778},
  keywords={Training;Degradation;Complexity theory;Image recognition;Neural networks;Visualization;Image segmentation},
  doi={10.1109/CVPR.2016.90}
}

@INPROCEEDINGS{Inception,
  author={Szegedy, Christian and Wei Liu and Yangqing Jia and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  booktitle={2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Going deeper with convolutions}, 
  year={2015},
  volume={},
  number={},
  pages={1-9},
  keywords={Computer architecture;Convolutional codes;Sparse matrices;Neural networks;Visualization;Object detection;Computer vision},
  doi={10.1109/CVPR.2015.7298594}
}