%-------------------------------------------------------------------
\chapter{Introduction}
\label{ch:intro}
%-------------------------------------------------------------------

This thesis is in the field of computer vision, specifically single image depth estimation.

\section{Background}
Under the name of "Depth Estimation" (DE) a lot of techniques appear, but there is no formal definition that considers them all.
I would define DE as the study of algorithms that process one or more images of a scene and output information about its geometry.
There exist many approaches to do this as well as many problem settings.
The setting of this thesis is \textit{Single Image Depth Estimation}, also called \textit{Monocular Depth Estimation}.
It refers to a depth estimation problem based on only one image of the scene.
As of today, this is done by assigning to each image pixel a depth value corresponding to the depth of the object that forms that pixel in the camera coordinate system.


\textbf{Images} are what we want to extract geometry informations from.
Images are represented as two (gray images) or three (color images) dimensional arrays of limited positive numbers.
Color images can be in turn represented in different color spaces.
Images can be resized through sampling and interpolation.
I will usually refer to an image using the letters $I$ and $J$.
A pixel is a location on an image and is indexed using a pair of numbers $(i, j)$, sometimes written $ij$ or it can be indexed using a single letter like $i$ or $p$.
For gray images the value of an image in a certain pixel is a single number, while for color images it is a triad of numbers corresponding to some color space representation, usually RGB.
$I_{ij}$ denotes such values.
$ij$ can be a non-integer pair of numbers, in such a case $I_{ij}$ will still make sense and a \textit{differentiable} sampling procedure is implied(usually the one used in \cite{STN}).


\textbf{Depth maps} are often the output of depth estimation algorithms.
A depth map is an image in which every pixel value represents a depth measure, meaning the distance from the image plane or, equivalently, the $Z$ coordinate of a point $\mathbf{x} = (X, Y, Z)$ in space, where $X$ and $Y$ corresponds to the image plane coordinates.
Since only points in front of the camera are considered, $Z$ values are always strictly positive.
I will usually refer to a depth map using the symbol $\mathbf{Z}$.
$\mathbf{Z}$ can be a \textit{metric} depth map, meaning each pixel value represents a spatial distance expressed in absolute physical units (e.g. meters), or it can be a \textit{relative} depth map and only used for relative depth comparisons of pairs of pixel (e.g. which pixel is closer or farther).
An important kind of a relative depth map is an \textit{affine} depth map, that is a depth map equals to the metric depth map of the scene up to an affine transformation: $\mathbf{Z}_{metric} = s \, \mathbf{Z}_{affine} + t$ for some $s, t$.

Following an image formation model, in both images and depth maps every pixel value is determined by some property of a 3D scene location which I will denote as $\mathbf{x}$, be it distance, reflectance, color, texture, ...
The camera acquiring the image has \textit{intrinsic} parameters describing its acquisition geomtry and \textit{extrinsic} ones refering to its position in space.
Projective geometry is used for expressing camera and image geometry \cite{multiview}.
I won't go into its details, suffice it to say that by knowing these parameters one is able to \textit{project} a 3D location $\mathbf{x}$ to the image plane and by knowing the distance(depth) $Z$ of such a point and its location $(i,j)$ in the image plane also the reverse is doable, namely \textit{backprojecting} the pixel to a 3D location.


\textbf{Disparity maps} are a concept related to the so called \textit{stereo} or \textit{binocular} depth estimation where scene geometry must be infered from two images.
Disparity maps will be indicated by $\mathbf{D}$. In order to understand what they are, assume there is an object in the scene at $\mathbf{x}$ and two identical cameras simulteneously take a picture from slightly different angles.
The object location in the first image $(i_{1}, j_{1})$ will be different from its location in the second image $(i_{2}, j_{2}) $(imagine overlapping the two images).
A displacement $d = (i_{2} - i_{1}, j_{2} - j_{1})$ is thus obtained.
By knowing extrinsic and intrinsic parameters of the two cameras, depth $Z$ from the first camera corresponding to $(i_{1}, j_{1})$ can be computed by means of such displacement.

Thus, for every pixel $(i, j)$ of the first image, by backprojecting it in 3D space and then projecting it on the second image a displacement $\mathbf{D}(i,j)$ can be computed.
The mapping from pixel to displacement is called a disparity map, in this example we computed the disparity map of the first image w.r.t. the second.
Analogously a disparity map can be computed for the second image w.r.t. the first.
There exist pixels for which this procedure fails either because the projection of $\mathbf{x}$ onto the other image ends up out-of-view or because $\mathbf{x}$ is occluded from the other perspective and does not contribute to image formation.
Hence disparity maps can have "holes" in which they are not defined.

In the very simple setting of identical cameras put side to side (mounted on a \textit{stereo-rig}) pointing at parallel directions, the disparity $d$ is a one dimensional displacement and is represented by a single real number.
This will be the case from here on. The obtained image pair is called \textit{rectified}.


The problem of depth estimation is to design an algorithm that can reconstruct the geometry of a scene from $N$ images of it.
A function $f$ is required such that $f(I_{1}, I_{2}, ..., I_{N})$ represents such geometry, usually as a depth map $\mathbf{Z}$ or disparity map $\mathbf{D}$, but other representations exist in particular when $N > 2$, i.e. \textit{multi-view} case.
There exist surveys treating the problem for $N \geq 2$ \cite{correspondance, stereo}, but the focus of this thesis is the case $N = 1$, namely \textit{monocular} or \textit{single image} depth estimation.


In the \textbf{deep learning} approach $f$ is called a $model$ and has \textit{trainable} parameters which must be tuned minimizing what is called a \textit{loss function} $\mathcal{L}$, that is: a quantity expressing some undesired property that $f$ mustn't satisfy.
For it to be minimized during a first phase called \textit{training}, differentiability of $\mathcal{L}$ in the trainable parameters is required.
This implies that $f$ itself should be differentiable in them.
Algorithms based on gradient descent are employed for optimization.
$\mathcal{L}$ is often expressed as a sum of other loss functions and so called \textit{regularization} terms which serve the overall training procedure, favouring the \textit{generalization} of the model.
A model is said to generalize well when its predictions are reliable on new input data, i.e. not seen during training.
This whole procedure is only possibile if a \textit{dataset} is given, that is a collection of data statistically defining the desired behaviour of $f$.
It can be a set of input-output pairs, e.g. images and associated depth maps, or a more general data collection, e.g. video footage.
The desired output of the model in response to a certain input is called \textit{ground-truth}.

\textit{Testing} follows training.
During the testing phase the model generalization capability is evaluated on new data using \textit{metrics} $\mathcal{M}$ which quantitatively express the \textit{error} the model is committing (like loss functions do, hence the smaller the better) or its \textit{accuracy} (the greater the better).
In order to test a model a dataset containing ground-truth data is necessary so that the desired behaviour can be quantitatively compared with the actual behaviour.
The output of a model is often called a \textit{prediction}, so metrics $\mathcal{M}$ are functions of both predictions and ground truth data.


\textit{Statistical learning} is the predecessor of deep learning and it shares with it the same theoretical foundations, but it proved to be less effective.
Deep learning efficacy is mainly due to its scalability in training, meaning that very large datasets can be used and the resulting optimization problem is computationally feasible for the existing hardware, although the costs are not always affordable.

%During testing time all monocular depth estimation models are able to infer depth from a single image, however during training the kind of data they require can vary from tecnique to tecnique.
%Chapter 2 treats these differences. In its "Supervised" section models trained on ground-truth data are presented, while in the "Self Supervised" one ground-truth is not necessary and data can be video footage or stereo pairs.
%Instead in "Classic" techniques not necessarily based on deep learning(and hence on datasets) are reviewed.
%
%In this chapter some of the ideas developed for solving monocular depth estimation are reviewed.
%The "Classic" section is about works that don't use what we'd call deep learning.
%"Supervised" , "Self Supervised" and in particular "SOTA" are solely about deep learning techniques.
%Datasets and metrics details are encapsulated in the "Datasets" and "Metrics" sections.
%
%
%I won't spend too much time on neural network architectures and in particular on explaining numerical details and experiments.
%If a method appears in this chapter it means that it was successful in its intent.
%The scale of efficacy is Classic $<$ Self Supervised $<$ Supervised $<$ SOTA.

