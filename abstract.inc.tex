\chapter*{Abstract}

This thesis is about single image depth estimation (SIDE), a subfield of computer vision, and interpretability of deep learning methods.

SIDE deals with the problem of reconstructing the geometry of a scene from a single image of it.
As of today, this is done by assigning to each image pixel a depth value corresponding to the depth of the object that forms that pixel in the camera coordinate system.

Deep learning models are known to be black-boxes, meaning that they are not understandable (\textit{interpretable}) by humans.
All "state of the art" approaches in SIDE are black-boxes.

This work has two objectives: to theoretically investigate interpretability of algorithms and to propose a methodology for achieving it in SIDE.
It is argued that there are upper limits to interpretability of algorithms solving certain tasks like SIDE, implying that a black-box component \textit{must} be employed in order to solve them.
The motivations for this are to be found in the incapacity of formal language to capture the complexity of human cognition.

For achieving more interpretable methods, deep learning has to be confined to sub-tasks.
SIDE is known to be decomposable into the following sub-tasks: (1) estimate depth maps of image patches and (2) merge them into a full prediction.
Performed experiments explore alternative ways of defining the first sub-task with the objective of further simplifying it.
This simplification requires an appropriate merging procedure.
An interpretable depth fusion procedure is proposed.

\thispagestyle{empty}
\mbox{}
\newpage
