%-------------------------------------------------------------------
\chapter{Methods}
\label{c:methods}
%-------------------------------------------------------------------

%\epigraph{\enquote{I believe that inside every tool is a hammer.}}{\emph{Adam Savage}}

\section{Thoughts on Interpretability}
In the fields of philosophy, psychology and cognitive science there are a lot of mature works addressing the problem of what an explanation is and how people evaluate its quality, but most of XAI research is based on an intuitive idea of what constitutes a “good” explanation because seeking explanations as defined by such social sciences would lead to failure \cite{Miller}.
There is an explanatory gap (EG) between natural language and mathematical language expressiveness which undermines the possibility to rigorously define what an explanation is in mathematical terms, necessary for applying it to artificial intelligence systems.
This same gap is also the reason for which training objectives expressed as mathematical functions do not account for the complexity of real-life goals involved in tasks such as driving \cite{Zablocki2022}.
Hence, research in the field of XAI is driven by mathematical heuristics that result in intuitively “good” explanations.
However, humans are known to be highly biased when taking their decisions intuitively rather than through critical analysis \cite{Kahneman}. 

To show how humans are not good at intuitively judging what a good explanation is, consider for instance the family of XAI post-hoc methods in computer vision that produce a saliency map (a.k.a. heat map) as explanation (e.g. GradCAM \cite{GradCAM}).
They are so-called local methods, meaning: given an input image $\mathbf{I}$ and a model $f$, they return an explanation of its specific prediction $y = f(\mathbf{I})$.
This explanation is a map assigning an “importance” value to image pixels.
It is easily visualizable (usually overlaid on the input image $\mathbf{I}$, see figure \ref{fig:grad_cam}) by a human who, by visual inspection, can immediately grasp which parts of the image are supposedly more relevant for that prediction by $f$.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figs/grad_cam}
    \caption{
        Image from \cite{GradCAM}.
        (a) Original image.
        (b) Saliency map obtained with another method.
        (c) Saliency map (with respect to the class "cat") obtained with GradCAM and overlaid onto the original image.
        \label{fig:grad_cam}
    }
\end{figure}

For example if the model solves a classification problem then the highlighted pixels represent “where it looked at” for taking the classification decision.
However, the explanatory gap of above is filled by human cognition.
Indeed, humans perceive images at first sight in a very rich way and decompose it directly into objects, materials, lights, shadows, reflections, geometry \cite{pinker2009mind}.
This implies that when looking at an image with highlighted areas (as it is when visualizing a saliency map) it is difficult to separate the rich conscious experience of the image from the actual information provided by the explanation.
The explanation has a very specific mathematical meaning, but it will look “good” as long as the highlighted area roughly contains reasonable parts of the image involved in humanly justifying the ground truth answer.
For instance, the model classifies an image as a cat and cat pixels are highlighted.
Although this kind of explanation is informative of the model behavior and can be 
effectively used for debugging purposes \cite{molnar2022}, it doesn't increase its interpretability.
It doesn't inform about how the model took its decision but rather which was it. 

All XAI methods suffer from similar problems: they provide seemingly “good” explanations, which appear so because of cognitive biases, but don't actually increase interpretability of the model on which they are applied because of the specific mathematical meaning of their explanations.
This is a consequence of the EG which limits human understanding of complex mathematical artificial intelligence models.
This can be alleviated by making assumptions about the targeted task.
The more assumptions are made when developing a model, the smaller the gap is.
In fact assumptions simplify reality, making it more easily representable in mathematical terms and leading to a better understanding of the resulting AI model.
Human tasks, namely tasks that humans can solve and a machine is asked to do as well, are the most susceptible to the explanatory gap.
Examples of them are: object detection, image segmentation, question answering, speech-to-text, driving.
Depth estimation is, generally speaking, a human task, but, if one thinks more carefully, it probably isn't.
It'll be discussed later.
For solving this kind of tasks, humans use their cognitive abilities and the tasks themselves are defined by those abilities.
Because it's difficult to abstract from their own cognition, when dealing with such tasks humans are more easily biased.
Also, simplifying the problem by making assumptions is hardly ever possible.

\vfill

The methods discussed in the previous section need a review after this discussion.
Let's start from the work of You et al. \cite{towards_interpretable}.
The authors assume that depth selectivity of feature channels is a measure of interpretability without explaining why.
They rather give general analogies with other works where a similar dissection of the network behavior was done, but for different tasks.
The proposed interpretability measure is, in my opinion, arguably a bad measure.
Consider a depth estimation model based on a bin method, in such methods there usually are "bin discriminating layers" (BDLs) that are specialized in classifying the input pixels with a bin label through the activation of a feature map.
Depth selectivity measures how much some layers of the model work like a BDL, so for instance a bin method in correspondence of BDLs would appear to be highly interpretable.
But BDLs are the second-to-last step in depth estimation based on bin methods, where the last is just an aggregation of the bin scores into a depth value.
Hence, for You et al., an interpretable network that performs depth estimation is just a network that solves the task in early layers, i.e. a smaller network.

In "Visualization of Convolutional Neural Networks for Monocular Depth Estimation" \cite{Hu}, Hu et al. define their explainability method (in this case a saliency method) based on its output.
They repeatedly notice that if the mask $\textbf{M}$ is not properly optimized the result will be "unexpected" or not "useful".
The mask is defined in such a way that the explanation is meaningful to humans.
So, while other formulations are mathematically equivalent for what concerns the main optimization objective, the regularization is chosen based on the resulting appearance.
Hence, the authors fall in a circularity.
Notice that their objective is not to output a good-looking image, but to investigate the behavior of the model.
While their visualization method can be useful, it is based on an arbitrary concept of "relevance" for pixels contributing to the prediction.

The first work discussed from Dijk et al. \cite{Dijk} is, in my opinion, a good investigation on what certain neural networks do.
It is not \textit{explaining} their behavior, but it is informative nonetheless and sheds light on various potential biases of models trained on the KITTI dataset.
For what concerns the current discussion I don't feel like criticizing it.

\vfill

\paragraph{Depth estimation and Human Tasks.}
Humans can perceive depth.
Formally (assuming a "formally" is even meaningful), in what sense humans perceive depth?
The following discussion is about personal observations based on introspection.

Humans don't perceive the environment neither in terms of pixels, nor through static entities like images are.
It is difficult to align human depth perception to the formulation of machine depth estimation.
Another aspect to consider is that in certain contexts humans can estimate distances, while in others they cannot.
Also: in certain contexts humans can estimate \textit{relative} distance of objects, while in others they cannot.

This implies that humans cannot annotate whole images with their metric depth map and neither with their relative depth maps.
The only kind of annotation humans can perform is that of DIW dataset \cite{DIW}, where humans were asked to tell which pixel was closer to the camera given another.

One could argue that given a digital image to a human, some software support and enough time, an \textit{accurate} dense depth map could be annotated.
For sure, humans are not able to grasp all the content of an image (particularly \textit{depth} content) by looking at it only once.
More looks are necessary and eyes must explore it also applying some sort of visual reasoning, e.g. visually linking objects to their orthogonal projection on some visible straight line for accurately estimating their depth.
Finally, as previously discussed in this chapter, there have been observed visual cues associated with depth perception in humans.
The role of these cues is still to be clarified since neuroscientific studies did not identify yet a global depth perception process that explicitly uses them.
But when it comes to visually reasoning about an image depth, these cues come handy to computational thinking. 
Nevertheless, assuming humans have the right utility instruments and are properly trained, it still has to be empirically proven that they are able to perform a meaningful depth estimation as machines do pixel-wise.

Hence, it can be safely stated that humans can perceive depth but, right now, they cannot perform depth estimation as formulated in computer vision.
Depth estimation is not a human task.

\vfill

\paragraph{The Necessity of Deep Learning in Depth Estimation.}
Let's rewind a little and go back to the concept of explanatory gap (EG).
A significant aspect of the EG is the fact that it is about the general difference in expressiveness of natural language and mathematical language, thus it is not only about explanations.
In order to clarify this point, consider that, when designing an algorithm for solving a certain \textit{problem formulated in natural language}, a developer represents the entities relevant to the problem and relations between them into mathematical language.
Since the natural language is more expressive than mathematical one, it could happen that some concepts cannot be faithfully translated into mathematical representations (as already noted for training objectives of complex learning algorithms and their explanations).
The intrinsic transparency of an algorithm comes from human interpretability of its passages which are supposed to represent meaningful operations.
Thus, a task involving high-level difficult-to-mathematically-represent requests will always have a certain degree of non interpretability.
This has a fundamental consequence on the whole field of XAI: interpretability methods cannot return arbitrarily good explanations for algorithms solving such tasks.
Equivalently, there is an upper limit to interpretability (hence also to transparency, since necessary to interpretability) for these algorithms.

Depth estimation (DE) suffers from this same problem.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
DE, as claimed above, is not a human task and is formulated in mathematical language.
But, even though humans are not able to solve DE, they \textit{implicitly} add constraints to the desired solution.
These constraints are not explicitly stated and involve the semantic of the scene.
In certain DE settings, like \textit{binocular} depth estimation (BDE), the problem can still be solved using geometrical reasoning (epipolar geometry \cite{multiview} for BDE\footnote{
    There are various situations in which BDE is not solvable using only geometric relations \cite{correspondence}.
    Usually due to reflections, high frequency textures, material transparency and occlusion.
    Refer to \cite{stereo} for a survey on deep learning based techniques.
    Notice, however, that deep learning does not substitute the previous techniques for BDE, it rather enhances them.
    In MDE a complete substitution happened.
}).
But, in the case of \textit{monocular} depth estimation (MDE), the problem is ill-posed.
The solution of an MDE instance must implicitly align with human semantic of the scene that cannot be encoded through mathematical language.
Hence, MDE is subject to the EG consequences on interpretability.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This undermines the possibility of actually developing a fully interpretable algorithmic MDE (since there is an upper limit to the interpretability of its algorithms).
This implies that a non-interpretable technique \textit{must} be used for solving the task.
So, deep learning, which by now it proved more effective than all other learning techniques, \text{must} be inserted in the depth estimation pipeline.
Deep learning is necessary.

Notice though that, while deep learning is necessary, it is not said that the entire procedure must be an end-to-end DL model.
There is room for complementary procedures, possibly interpretable, that support the learned models.
This is where my personal experimental contribution comes in place.

\section{My approach to Interpretable DE}
MDE is not a human task and hence humans cannot solve it.
Nevertheless, they perceive scene depth and reason about it.
Since an interpretable algorithm is, by definition, an algorithm which passages are human understandable, an interpretable MDE algorithm must be inspired to how humans reason about scene depth.
As stated before, MDE cannot be solved using a \textit{fully} interpretable procedure (which would also contradict the fact that MDE is not a human task), hence black-box models must be used in the overall pipeline.
A hybrid single image DE pipeline must be developed.

\subsection{Proposed DE Pipeline}
The developed pipeline can be applied to any input image, but experiments were done on the KITTI dataset \cite{KITTI}.
Given a tensor image $\mathbf{I}$ of shape $H \times W \times 3$ as input, a criterion is used for sampling patches $\mathbf{P}$.
All patches must have the same shape $h \times w \times 3$.
Depth is estimated from each patch through a black-box algorithm and the results are stitched together in the final output depth map.
For this purpose, patches cannot be too small since depth can be successfully estimated only if global information is provided.
On the contrary, patches too large make the stitching phase not necessary and the whole pipeline would reduce to a non-interpretable black-box prediction.
By visually inspecting various crops from KITTI images, I found patches of width and height of 150 to be sufficient for humanly perceiving depth.
Empirical experiments must be further conducted for assessing this fact from a machin point of view, it is thus left to future works.

\vfill

\paragraph{Sampling and Training.}
The patch sampling is used in two different ways: for deciding on which patches the model will be trained on and which patches are to be used for inferring the full image depth map. 
During training the sampling procedure must ensure enough data diversity which act as a regularization, while during inference it should target patches expected to produce confident predictions.
A lot of design choices can be made to this extent.
I experimented with two sampling procedures:
\begin{itemize}
    \item{
        \textit{Random strategy}: during training, patches are randomly cropped, while during inference a grid of overlapping patches is deterministically produced.}
    \item{
        \textit{Corner strategy}: image corners are detected using a corner response function.
        During training patches are randomly sampled around corners and during inference only the corners with higher response are considered.
    }
\end{itemize}
The sampling procedure is a filter to which data the black-box model is trained on.
The corner strategy defines a smaller data manifold to train on than the random strategy does.
By making the data manifold smaller, the learning problem is simplified since it is forced to learn fewer relations.
This holds as long as the smaller manifold is a subset of the bigger manifold being compared to.
The corner strategy effectively defines a subset of the patches the random strategy samples.
Even though the black-box model still lacks of interpretability, it is easier to build trust in it since it tackles a simplified version of the original problem.

The corner strategy was inspired by observing how the human eyes move when looking at images.
They continuously change their point of focus and rarely lay on plain regions.
They rather stop on what in computer vision are called corners and edges.
In here I choose to work only with edges, a more general sampling strategy is left as future work.

The corners are detected with a classic corner detector from Moravec, but others can be employed \cite{computer_vision}.

\paragraph{Patch pre-processing and Training Loss.}
Sampled patches are further processed before being fed to the network so to further simplify the learning problem.
I experimented with two transformations: warping the patch to preserve camera geometry and radially blurring it.

The warping procedure ensures that each sampled patch is captured by a virtual camera with its principal point in the center of the patch.
Camera parameters are needed in order to do this.
The procedure is the following:
\begin{enumerate}
    \item{Sample a point $p$ from image $\mathbf{I}$.}
    \item{Compute the camera transformation that maps $p$ to the principal point.}
    \item{Warp the image using the computed transformation and obtain a new image $\tilde{\mathbf{I}}$.}
    \item{Crop $\tilde{\mathbf{I}}$ around the principal point.}
\end{enumerate}
This procedure can produce patches with out-of-view pixels, to mitigate this, in the sampling procedure strips along the borders are ignored.
Moreover, the warping can produce lower quality images since a bilinear interpolation step is required.
Instead, for producing the depth map of the patch, the point cloud provided with the image is projected onto the image, coherently with the computed transformation.
I experimented also with simple image crop, with no warping.
This warping procedure simulates patches captured with identical cameras and imitate human eyes that rotate (as the camera here does for warping) for observing different visual regions.

After having cropped (and optionally warped) the patch, I experiment with radially blurring it\footnote{It is not a proper radial blur, but it produces a similar visual effect.}.
The blur effect is given by iteratively applying gaussian blurring to the patch $\mathbf{P} = \mathbf{P}_{0}$, obtaining $\mathbf{P}_{1}, \mathbf{P}_{2}, \dotsc, \mathbf{P}_{N}$.
Each blurred patch is center-cropped.
To the most blurred one $\mathbf{P}_{N}$ no cropping is applied, to $\mathbf{P}_{N-1}$ a center crop slightly smaller than the previous crop is applied.
And so forth.
Are obtained images of various decreasing sizes:
\[
    w = w_{N} \, > \, w_{N-1} \, > \, \dotsc \, w_{1} \, > w_{0} \, > w_{min}
\]\[
    h = h_{N} \, > \, h_{N-1} \, > \, \dotsc \, h_{1} \, > h_{0} \, > h_{min}
\]
Where $w_{min}$ and $h_{min}$ represents the minimum shape of the non-blurred area.
The radially blurred patch is obtained by concentrically overlapping this patches on top of each other, starting from the most blurred one.
Radial blur causes some information to be lost.
A model trained on such blurred images, but asked to perform equally well on the whole image, is likely to see a degradation in its performance because there is less information available as one gets away from the center.
Only in particular circumstances it would imply an improvement (if the model overfits to details near image boundaries).
For this reason I employ both the metrics from table \ref{t:metrics} and a modified version of them that weights the contribution of each pixel based on its distance to the center of the patch.
The weighting uses a gaussian density function with standard deviation heuristically set to $(w + h) / 2$.
% This ensures that pixel coordinates are also properly scaled in computing the distance from the patch center.

A similar modification was applied to the loss employed during the training phase.
A radially weighted scale-invariant-loss is used (the unmodified version is from \cite{Eigen}).
This loss encourages to better-learn depth values around the center of the patch.
The blurring smooths out the data manifold by reducing the space of patches considered.
The informal reason for this is that patches with high-frequency details around the border are mapped to lower-frequency versions, which are less numerous in this finite sampling context.

\section{Depth fusion pipeline}
After having gathered partial depth predictions of the input image, an algorithm for composing a final unique estimation is needed.
Here, a theoretical discussion is made and a pipeline is designed.
Experiments will follow in future works.

The depth fusion part must be as interpretable as possible since a black-box model has been employed for the DE part.
In the section \ref{s:miscellaneous}, a tile-based method for DE was discussed: PatchFusion \cite{PatchFusion}.
While extremely powerful, it is an end-to-end non-interpretable deep learning model that estimates depth patch-wise and subsequently merge them.
% BoostingDepth \cite{BoostingDepth} instead uses a two-stage method with a pre-trained MiDas \cite{MiDas} model for depth estimation and a UNet architecture for merging, but still the overall algorithm is not interpretable.
% The older works from Konrad et al. \cite{konrad2} and Karsch et al. \cite{DepthTransfer}, and a very recent work from Rey-Area et al. \cite{360MonoDepth}, use handcrafted pipelines for the depth fusion part.
% Except for Konrad et al. who take the median of the depth values to be aggregated, the others set up an optimization problem describing desirable properties the resulting full-resolution depth map should satisfy.
I propose to perform a fine-tuning of the black-box model for merging the estimated patches.
Explicit formulation of the tuning objectives is needed, but the knowledge of the network is exploited for making the depth adjustments rather than explicitly parametrizing them (as other works instead do \cite{konrad2, DepthTransfer, 360MonoDepth}).
One could argue that during inference time there is no ground truth available and hence the tuning would be arbitrary.
But, since the model was trained with a loss that put more emphasis on certain areas of the model input, this information could be exploited by treating those areas as ground truth.

The following is inspired to how humans analyze images: they explore them with the eyes building "paths" to follow for visual reasoning and, sometimes, retracting their conclusion by exploring the image differently.

\vfill

Let $\mathbf{I}$ be the input image, $\mathbf{P}_{0}, \mathbf{P}_{1}, \mathbf{P}_{2}, \dotsc, \mathbf{P}_{N}$ the patches and $\mathbf{D}_{0}, \mathbf{D}_{1}, \mathbf{D}_{2}, \dotsc, \mathbf{D}_{N}$ their estimated depth maps.
A pixel $p \in \mathbf{P}_{n}$ corresponds to a pixel $\hat{p} \in \mathbf{I}$ and, if some overlap occurs, it corresponds also to a pixel $\hat{p}_{m} \in \mathbf{P}_{m}$.
All the patches are projected onto the image $\mathbf{I}$ so that pixel coordinates live in the same space.
Hence, a same pixel $p \in \mathbf{P}_{n}$ belongs to $\mathbf{I}$ and can also belong to other patches $\mathbf{P}_{m}$.
The black-box DE model is denoted $f$.
A copy of $f$ is used and denoted $\hat{f}$, this will undergo a fine-tuning stage.
Let's call $\hat{\mathbf{D}}_{0}, \hat{\mathbf{D}}_{1}, \hat{\mathbf{D}}_{2}, \dotsc, \hat{\mathbf{D}}_{N}$ its predictions on the considered patches.

By fine-tuning $\hat{f}$ is order to make the partial predictions agree on the overlapping regions is a first approach.
This fine-tuning happens each time inference is run.
Hence, $f$ is frozen and preserved.
The following loss could be minimized:
\[
    \mathcal{L} =
        \sum_{n \neq m} \,
        \sum_{p \in \mathbf{P}_{n} \cap \mathbf{P}_{m}}
        e^{-\mathbf{W}_{n}(p)}
        (\hat{\mathbf{D}}_{n}(p) - \mathbf{D}_{m}(p))^{2}
        \, + \,
        \lambda
        \sum_{n}
        \sum_{p \in \mathbf{P}_{n}}
        \mathbf{W}_{n}(p)
        (\hat{\mathbf{D}}_{n}(p) - \mathbf{D}_{n}(p))^{2}
\]
Where $\mathbf{W}_{n}$ is a weight mask, as used in the weighted scale-invariant loss and the weighted metrics, that fades radially (if warping was applied to the patch, also the mask is warped).
$\mathbf{W}_{n}(p)$ is a measure of how much the model \textit{should} be confident about the prediction in $p$.
Pixels with high confidence contribute less to the loss.
The first term of the loss encourages different depth maps to agree on the overlapping regions, while the second one avoids the network to completely forget its initial prediction and degenerating into a constant solution. 
$\lambda$ is a hyperparameter.

The overlapping between patches need to be monitored since it could create an unbalanced loss.
For this reason the above loss require further modifications.
Instead of optimizing all patches together, it could be established an ordering.
Also, the model will perform differently on different patches and so not all patches should be treated equally.
A procedure similar to the one used for variable selection in statistics could be employed.
Hence, the optimization would involve an initial small set of patches to which the other ones are iteratively added and optimized, without necessarily using all of them (forward selection).
Or, some could be iteratively removed from the whole set (backward selection).

Assume there is a strategy for choosing one or more initial patches in which the model is more confident, i.e. some error measure w.r.t. ground truth is \textit{probably} lower.
For instance, in \cite{BoostingDepth} it is observed that depth estimation models better perform on regions with a precise \textit{edge} density that depends on the dataset the model was trained on.
A simple criteria could be to firstly select the patch with edge density closest to the reference value.
Let's call $n_{0}$ its index.
Consider now another patch $\mathbf{P}_{n_{1}}$.
If it overlaps with $\mathbf{P}_{n_{0}}$ a comparison can be made and might be used for establishing whether to include it in the optimization process or not.
Ideally, assuming DE is \textit{metric} (otherwise an affine correction is required), patches with a large overlap should agree on depth values, while patches with little overlapping may disagree more.
This is due to the radial weighting applied during training and to a general translational invariance of convolution layers of the black-box\footnote{Their presence is not obvious, but it is extremely common. If they are not present, data augmentation should provide such an invariance.}.
For building a coherent depth map, the second patch must present a compromise between fully overlapping and not overlapping, but it should agree as much as possible with the initial patch.
After the patch is selected it is merged with the previous one, creating a new patch.
If a criterion that fulfills these requirements is defined, it could be used to iteratively add and optimize patches until a condition is met.

It could happen that some non-optimal choice has been taken during the process.
In this case, a backtracking procedure should be considered for producing an alternative merge.
For successfully applying such a thing, fine-tuned models must be copied each time, producing models $\hat{f}_{0}, \hat{f}_{1}, \dotsc$ corresponding to each iteration of the merging procedure.
This whole fusion pipeline seems computational expensive, but if the neural network used for DE patch-wise is lightweight (and that is the objective of the previous paragraphs) this pipeline is feasible.

%\subsection{Motivations for the proposed pipeline}
%By introspection, I observed the following inner facts about scene depth perception (and reasoning) from single image:
%\begin{enumerate}
%    \item{
%        Depth information of what I am looking at is immediately provided;
%    }
%    \item{
%        When I am looking at something, I am not looking at other things, even if close to it;
%    }
%    \item{
%        The eyes continuously move across the image;
%    }
%    \item{
%        The eyes visit more often ambiguous regions;
%    }
%    \item{
%        Sometimes, the eyes move following precise patterns;
%    }
%    \item{
%        Depth perception of things can change throughout the image analysis;
%    }
%    \item{
%        Explicit visual reasoning is performed using eyes movement;
%    }
%    \item{
%        The eyes tend to look at corners (in the computer vision sense, refer to \cite{computer_vision});
%    }
%    \item{
%        When the eyes look at an edge, they tend to follow it in subsequent movements;
%    }
%    \item{
%        When the eyes look at a texture-less region, they tend to move randomly in it.
%    }
%\end{enumerate}
%The fusion pipeline and experiments formulated above are inspired by these observations.
%The non-interpretable component, in my opinion, must be inserted in the immediate (partial) perception of depth of things (point 1 of the bullet list).
%By things, I don't mean well-formed objects.
%Instead, I refer to places where the eyes stop.
%Point 2 sounds tautological, but it is fundamental.
%It states that when looking at something, even though other things fall into the visual field, no depth information about them is provided.
%Mathematically, it is difficult to express this fact.
%The way I choose to give it a formalization is: the reliability of pixels in an atomic depth estimation decreases radially.
%In this way, for having a reliable depth map of a full image, multiple "looks" at it are necessary.
%This aligns with the tile-based approaches to MDE and with the design choices I made.