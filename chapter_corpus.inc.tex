%-------------------------------------------------------------------
\chapter{Corpus}
\label{c:corpus}
%-------------------------------------------------------------------

%\epigraph{\enquote{I believe that inside every tool is a hammer.}}{\emph{Adam Savage}}

\section{Deep Learning and Interpretability}
Since the advent of machine learning (ML) and artificial intelligence (AI), there has been a paradigm shift in solving problems in a lot of fields.
Instead of designing an algorithm that directly solves a given task, a learner is delegated to find a good-enough solution in a pool of possible solutions by solving an optimization problem defined on a set of data points \cite{ML_book}.
Among all, deep learning (DL) techniques are the most successful ones and have a much superior performance than the statistical learning counterparts in the most challenging tasks (vision, language, autonomous agents, …), provided data is enough.
Deep learning is more scalable w.r.t. dataset size and data dimensionality than statistical learning techniques are and is better suited for exploiting the existing hardware, which in turn was improved for the purpose during last years.
This made DL solutions successful, popular and spreading to more and more research fields, supported by an ever-growing data and hardware availability \cite{DL_overview}.

Although deep learning models are very effective, it is very difficult for a human (experts included) to understand how these models come up with their outputs due to their large sizes, the (general) high-dimensionality of their inputs and training algorithms.
For they are considered black boxes \cite{DL_overview}.
Indeed, humans understand which mathematical operations DL models carry out in order to compute numerical outputs, but they are not capable of intuitively interpreting those computations so that they represent something meaningful for them.
But why should humans be interested in understanding these models? 
If a model has been experimentally proven to be reliable, it is not involved in critical scenarios or, it has a limited impact on people and things, then not understanding its functioning doesn't raise concerns.
Yet, there are a lot of applicative contexts in which this is relevant, e.g. autonomous driving \cite{Zablocki2022}, Industry 4.0 \cite{XAI_industry} and healthcare \cite{XAI_healthcare}.
More generally, when the formal numerical objectives and metrics of a learning algorithm are not rich enough to describe real-world deployment settings, the need for a more thorough understanding arises \cite{Lipton}.
This, combined with the integration of AI in everyday life, results in society, ethics and legislation demanding a new generation of AI whose functioning is more transparent.
This trend is also observed in the rising number of publications during the last decade \cite{XAI_review} in the field of "eXplainable" Artificial Intelligence (XAI) which addresses this exact matter, particularly in relation to ML and DL.

The main property of an AI system studied by this field is its \textit{interpretability}.
This term is not rigorously defined since each author gives his/her own specific definition \cite{XAI_review}.
For example Miller \cite{Miller} defines interpretability as the degree to which humans can understand the cause of a decision made by an artificial intelligent system (its numerical output). 
While, Kim et al. \cite{examples_enough} give a more operative definition of interpretability as the capacity of humans to correctly and efficiently predict the AI system results, yet a lot of other definitions and characterizations have been proposed, e.g. see \cite{Lipton}.
Without limiting the discussion to a precise definition, interpretability can be thought as an intuitive understandability of an AI system and the way it produces predictions.
XAI addresses the problem of developing methods for generating explanations accounting for such predictions (post-hoc methods) or design principles to build systems intrinsically more interpretable (ante-hoc methods).

There exist some intrinsically interpretable models such as sparse linear regression, shallow decision trees or K-nearest neighbors \cite{molnar2022} \cite{XAI_review} that because of their limited size and mathematical complexity are easily understandable by a human expert but suffer from low accuracy, especially when tackling complex tasks.
In fact, a general trade-off between accuracy and transparency is known to exist in AI.
XAI studies models that traded off transparency for high accuracy by developing methods for clarifying their behavior after training has been performed (post-hoc interpretability methods), or aims at designing models that try to alleviate the limitations of that trade-off in advance (ante-hoc interpretability methods), for instance by being trained to return both a prediction and an explanation for it. 

There are other criteria for classifying interpretability criteria \cite{molnar2022}.
Distinctions can be made based on what kind of result an interpretability method returns.
For example there are methods that return counterfactual explanations for justifying a prediction.
If the model $f$ on the input $x$ returned the output $y = f(x)$, a counterfactual explanation would be an input $x'$ close to $x$ such that $f(x') = y' \neq y$ \cite{Zablocki2022}.
This explanation attempts to answer the question “What should have happened instead of $x$ so that $y'$ occurred?” by either finding $x'$ in the training dataset or by generating it trough generative AI techniques \cite{Zablocki2022}.
Some methods return a natural language explanation, this is the case for Xu et al. \cite{9157111} who developed a DL model for an autonomous driving toy setting producing both an output and a natural language explanation for it.
This was made possible thanks to annotated examples in the dataset employed. 

Another distinction can be drawn between so-called model-specific and model-agnostic methods \cite{molnar2022}.
Some interpretability methods are developed for a specific family of ML or DL models (e.g. Deconvnet \cite{Deconvnet}) leveraging on their specific mathematical structure and are thus named model-specific methods.
Other methods treat the model as a black box to probe, trying to model its behavior in a simpler way, an example of this is LIME \cite{LIME} which tries to locally linearly approximate the model by fitting a sparse linear regression model.
The fitted transparent model constitutes the explanation.

Finally, interpretability methods can be classified as local or global.
Local methods address interpretability of a single prediction of the studied AI model (i.e. around a single data point $x$), while global methods take into account the overall model behavior.
The above-mentioned methods are local.
A global method can consist in a feature selection procedure for identifying a subset of the input features that have more predictive power w.r.t. the others \cite{molnar2022}.
Global methods are more common in ML methods rather than DL ones. 

Lipton \cite{Lipton} identifies five aspects of an ML model on which XAI can shed light on.
These are:
\begin{itemize}
\item{\textbf{Trust}: If confidence that the model properly performs in real-world scenarios is 
required and numerical assessments of its performance are not sufficiently 
expressive, then trust has to be built in other ways, a more deep understanding of 
the model helps in this direction. Trust is especially needed in critical applications 
like in medical decision-making \cite{XAI_healthcare} and autonomous driving \cite{Zablocki2022}, where extensive 
testing is difficult to achieve due to safety, economical or legal concerns.}
\item{\textbf{Causality}:
Especially in scientific research, where ML models are a way to extract information about the world, relations inferred by the these methods are not guaranteed to reflect real causal relationships.
By understanding the fitted models scientists could gain new insights and test them through new experiments.
}
\item{\textbf{Transferability}:
It is common that training settings do not match deployment environments due to their complexity or non stationarity.
Robustness to changes is a desirable property in this case and particularly desired in industrial settings \cite{XAI_industry}. 
Understanding the AI model can serve the purpose of inspecting how suitable it is for deployment.
}
\item{\textbf{Informativeness}:
Even though understanding an artificial intelligence model is sometimes very difficult (in particular neural networks), trying to do it can nevertheless convey useful information.
For instance, an explanation to the output of a diagnosis model can highlight clinical details missed by a doctor \cite{XAI_healthcare}.
}
\item{\textbf{Fair and Ethical decision-making}:
Since ML models learn from data and data reflect society biases, these are likely to be biased as well.
For example this happens for language models, algorithms for recruitment or for granting loans. 
Only a thorough investigation of the AI system functioning is able to reveal these biases.
}
\end{itemize}
By inspecting an AI model with the above-mentioned interpretability techniques, a human can acknowledge the presence or absence of properties related to these five aspects.

\section{Single Image Depth Estimation and Interpretability}
There are a lot of works about interpretability of neural networks for vision, but the most studied tasks are image classification, object detection and image generation.
Despite the growing interest for monocular depth estimation, driven also by its relevance in robotics and autonomous driving applications, its interpretability side is yet to be deepened.
I will review here three relevant works on interpretability of single image depth estimation models that tackles the problem from different perspective:
\begin{itemize}
    \item{Dijk et al. \cite{Dijk} probe neural depth estimators in search for correlation with human visual cues;}
    \item{Hu et al. \cite{Hu} develop a (neural) method for visualizing relevant pixels for the estimation process;}
    \item{You et al. \cite{towards_interpretable} formulate a regularization term aiming at both measuring and increasing interpretability of the models.}
\end{itemize}

\subsection{Dijk et al.}
Depth perceptions in humans has been widely studied.
Human visual depth cues associated with full scene understanding by humans are divided into primary cues and secondary ones \cite{monocular2024}.
The primary cues, as reported in \cite{monocular2024}, are:
\begin{itemize}
    \item{
        \textit{Relative size} refers to the apparent difference in the area occupied by the projection of objects of physically similar size on the retina.
        In other words, between two similar objects, the one that seems smaller is farther from the observer.
    }
    \item{
        \textit{Relative density} is detected when there is a cluster of similar objects or textures.
        As the distance from the observer increases, the objects appear to be closer to each other and the textures become denser.
    }
    \item{
        \textit{Height in the visual field} is connected with the relation of the bases of objects, assuming the presence of a ground plane and the absence of a ceiling.
        Specifically, objects at a greater distance from the observer tend to appear closer to the horizon line.
    }
    \item{
        \textit{Aerial perspective}  appears when the air has a high concentration of moisture, pollution, or both, and the objects in the distance become bluer, have less contrast, or both in comparison to those in the foreground.
    }
    \item{
        \textit{Motion Perspective} is the phenomenon that appears when there is relative motion between the observer and observed objects.
        Objects that appear to be moving faster are closer to the observer.
    }
    \item{
        \textit{Convergence} is connected with the angle between the optical axes of the two eyes.
        If an object is far from the observer, the optical axes tend to be parallel and the angle between them is close to zero degrees.
        As the object distance decreases, the two axes converge to a point on the object.
    }
    \item{
        \textit{Accommodation} is the eye function in which the lens (which is essentially an asymmetric biconvex lens) changes its shape to enable focusing at different distances.
    }
    \item{
        \textit{Binocular Disparity} the bases for what is called stereo vision, is the difference in the location of an observed object in the two eyes and only has meaning if more than one viewing sensors (eyes in this case) are available.
    }
\end{itemize}
Secondary cues are instead texture gradients, linear perspective, brightness and shading, kinetic depth, kinetic occlusion, and gravity.
These cues are “secondary” because they are either a combination of the main or they are incorrectly taken into account.
Secondary cues are particularly relevant for monocular depth estimation since the primary cues mainly rely on both of the eyes.
An illustration of some of these cues can be found in figure \ref{fig:cues}.

% human depth cues
\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{figs/cues}
    \caption{
        Image from \cite{monocular2024}. Visual depth cues: (a) occlusion, (b) relative size, (c) height in the visual field, (d) relative density, (e) aerial perspective, (f) binocular disparity, (g) accommodation, (h) convergence, and (i) motion perspective.
        In cases (a)-(e) and (i) left side is the 3D representation of two objects and the right side is the corresponding 2D projection attained by a monocular observer.
        \label{fig:cues}
    }
\end{figure}

In \cite{Dijk}, Dijk et al. investigate the use of apparent size of objects and their position in the image by neural model estimating depth from single image.
They focus only on these cues since they argue that in the KITTI dataset, which is their test ground, the others are not relevant.
For instance, aerial perspective is not observed in KITTI images due to relatively low depth values while on a dataset like Cityscapes it is an observable atmospheric effect.

Given an image depicting an object of well known size and lying on the ground, it is possible to estimate its depth from its apparent size or its position w.r.t. the horizon.
Camera pose and parameters must be known to this extent.
In the first case, the object apparent size is inversely proportional to its depth, while in the second case the higher the object appears in the image (i.e. closer to the horizon line), the farther it is.
Depth values estimated in these two ways do not necessarily coincide.

\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{figs/object_disappearing}
    \caption{
        Image from \cite{Dijk}.
        Objects that are not found in the training set are not reliably detected if hard-pasted into the image.
        \label{fig:object_disappearing}
    }
\end{figure}

The authors of the papers use this fact to probe the network and measure the variation in depth across a set of hand-crafted examples.
They put a car of fixed size in different points of the image, corresponding to different depth points in real-world, and also put the same care in a fixed spot but varying its size.
What Dijk et al. observe is that neural networks rely primarily on the vertical position of objects rather than their apparent size \cite{Dijk}.
This is due to the fact that the depth of the manually inserted car was independent on its size, but it seemed to be completely determined by its vertical position.

The use of vertical position as a depth cue implies that the considered networks have some knowledge of the camera pose.
Since the KITTI dataset exhibits an approximately constant camera pose (except for some frame where the recording car bumps into some sloped terrain), a natural question is whether the network assumes the camera pose constant or it estimates it image-wise.

Again, the authors design some experiments that show something in between: the camera pose is generally under-estimated when either a pitch or a roll in the camera happens.


\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{figs/object_appearing}
    \caption{
        Image from \cite{Dijk}.
        A shadow is added to the bottom of the objects.
        They are detected, although their shape is not properly recognized.
        \label{fig:object_appearing}
    }
\end{figure}

The last matter Dijk et al. investigate is object recognition.
A first focus is put onto color and texture and, by passing to the networks images with altered coloring and/or textures, they observed that depth estimation models seem to be invariant to image coloring as long as texture is preserved.
More interestingly, they studied how the networks detect obstacles.
They observed the following strong bias of models trained on the KITTI dataset: objects that cast a strong dark shadow on the ground are detected and their depth is given by the position of their shadow (coherently with what observed in the experiments on depth cues).
In figure \ref{fig:object_disappearing} it can be seen that the presence of new objects is completely ignored in the respective depth map.
Instead, in figure \ref{fig:object_disappearing}, after the adding of theirs shadows, the considered objects appear in the depth map.

\subsection{Hu et al.}
Hu et al. \cite{Hu} investigate single image depth estimation models by attempting to visualize which pixels were relevant for the prediction.
As they point out, there are popular and arguably useful methods for producing visual explanations of image classification models predictions such as CAM \cite{CAM} and GradCAM \cite{GradCAM}, but they cannot be directly applied to CNNs performing depth estimation.
This holds true also for other explainability methods.

Thus, the authors employ another approach for visualization.
They formulate the problem of identifying relevant pixels as a problem of sparse optimization \cite{Hu}.
The underlying assumption made in this work is that  CNNs can infer depth map equally well from a properly selected subset of pixels of the input image.
Let's call $\mathbf{I}$ the input RGB image tensor of shape $H \times W \times 3$, $f_{pred}$ is the network and $\mathbf{Z} = f_{pred} ( \mathbf{I} )$ is the predicted depth map.
The objective is to specify a mask $\mathbf{M}$, that is a binary tensor of shape $H \times W \times 1$ that specifies which pixels are important in the image.
$\mathbf{M}(p) = 1$ means that $p$ is relevant for predicting the depth on that image, $\mathbf{M}(p) = 0$ otherwise.
By "relevant", Hu et al. mean that $f_{pred} (\mathbf{I}) \approx f_{pred} (\mathbf{I} \, \odot \, \mathbf{M})$, where $\mathbf{I} \, \odot \, \mathbf{M}$ is performed by tensor broadcasting.
Other works tried this same approach for producing useful relevant-pixels visualization for other computer vision tasks.
The main problem is how to find a suitable mask that is meaningful to humans.
Regularization terms constraining the mask can be specified for achieving this, but the choice here is to produce the mask through another neural network, constraining it to a certain manifold specified by the new model.

\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{figs/hu}
    \caption{
        Masking method from \cite{Hu}.
        Notation is different: $G$ corresponds to $f_{mask}$, $N$ to $f_{pred}$ and $\hat{Y}$ to $\hat{\mathbf{Z}}$.
        \label{fig:hu}
    }
\end{figure}

Let's call $f_{mask}$ the neural network that takes as input the image $\mathbf{I}$ and outputs the mask, formally $\mathbf{M} = f_{mask}(\mathbf{I})$.
A representation of the pipeline can be found in figure \ref{fig:hu}.
For making things differentiable, the mask is treated as a continuous object with values in $[0, 1]$.
The continuous masks will be thresholded during the visualization phase.
The network $f_{pred}$ is frozen during the training of network $f_{mask}$ and the loss function used is:
\[
    \mathcal{L}_{dif} (\mathbf{Z}, \, \hat{\mathbf{Z}}) +
    \lambda \frac{1}{HW} \big\| \mathbf{M} \big\|_{1}
\]
Where $\hat{\mathbf{Z}} = f_{pred}(\mathbf{I} \, \odot \, \mathbf{M})$, $\lambda$ a hyperparameter and $\mathcal{L}_{dif}$ is a measure of difference between the two predicted depth map.

By visually investigating the obtained masks, Hu et al. discover that the mask tend to be similar to the edge map of the image.
Though there are differences.
First: not all edges are relevant for depth estimation, it can be seen in the first image of the figure \ref{fig:pixel_relevance} that edges corresponding to white striped on the street are masked out.
Second: masks tend to be "thicker" than edge maps, in the sense that there are some filled regions.
In the case of the KITTI dataset (as depicted in the figure \ref{fig:pixel_relevance}), the region corresponding to the vanishing points of the scene seem to be particularly important for the model considered.
The authors performed various experiment also on the NYU-v2 dataset and by varying the hyperparameter $\lambda$ which controls the sparseness of the mask.

\begin{figure}
    \centering
    \includegraphics[width=1.\textwidth]{figs/pixel_relevance}
    \caption{
        Masks from \cite{Hu}.
        Top row: original images from KITTI.
        Middle row: edge maps.
        Bottom row: predicted masks.
        \label{fig:pixel_relevance}
    }
\end{figure}

\subsection{You et al.}
A more recent work is that of You et al. \cite{towards_interpretable}\footnote{With a title ("Towards Interpretable Deep Networks for Monocular Depth Estimation") very similar to the one of my thesis...}.
In their work they try to quantify the interpretability of neural networks for monocular depth estimation by measuring depth selectivity of their units.
They also propose a regularization term for training such networks that enhance their interpretability, as defined by them, and do not harm performance.

The main idea is to measure the depth selectivity of deep feature maps channels.
Namely, the tendency of a channel to have high activation in correspondence of pixels associated to a specified depth range.

Formally, consider an input image $\mathbf{I}$, the ground truth depth map $\mathbf{Z}^{*}$ and the intermediate feature maps $\mathbf{F}_{l}$ labeled by the respective layer $l$.
Each feature map tensor has shape $H_{l} \times W_{l} \times C_{l}$, as $l$ increases (i.e. deeper feature maps are considered) $H_{l}$ and $W_{l}$ decrease and $C_{l}$ increases.
You et al. consider the "response" $R_{l, c}^{b}$ of a certain channel $c$ at a certain depth $l$ w.r.t. a depth range $b$.
In order to do so, first the depth range is divided into $B$ (uniform) bins.
Second, given a certain input-output pair $(\mathbf{I}, \, \mathbf{Z}^{*})$ a mask that identifies which pixels fall into the $b$th-bin is defined:
\[
    \mathbf{M}_{b}(i, \, j) \, = \, \begin{cases}
        1 & \text{ if } \mathbf{Z}^{*}(i, \, j) \in b \text{th-bin} \\
        0 & \text{ otherwise}
    \end{cases}
\]
Now, to get the response $R_{l, c}^{b}$ of the channel $c$ at depth $l$ with respect to the depth range $b$, the $c$th-channel of the feature map $\mathbf{F}_{l}$ is averaged by weighting it with the computed mask $\mathbf{M}_{b}$.
Before computing such a weighted average, the $c$th-channel of the feature map $\mathbf{F}_{l}$, of shape $H_{l} \times W_{l}$, is up-sampled to $\mathbf{F}_{l, c}^{\uparrow}$ of shape $H \times W$.
The response is averaged across the whole dataset to get the final response $R_{l, c}^{b}$.

Hence, a response is a number. The higher the number and the more probable it is for neurons at channel $c$ at depth $l$ to fire in correspondence of pixels of depth in the $b$th bin.
A channel is depth selective if it fires only w.r.t. a certain depth bin.
To quantitatively measure the depth selectivity of a channel $c$ at depth $l$, $DS_{l, c}$, the authors use a formula that comes from neuroscience \cite{towards_interpretable}.
Some preliminary definitions:
\[
    R_{l, c}^{max} = \text{max}_{b \in [B]} R_{l, c}^{b}
\]\[
    \bar{R}_{l, c}^{-max} = \text{mean}_{R_{l, c}^{b} \neq \, R_{l, c}^{max}} \, R_{l, c}^{b}
\]
Thus the depth selectivity is defined as:
\[
    DS_{l, c} = \frac
        {|R_{l, c}^{max}| - |\bar{R}_{l, c}^{-max}|}
        {|R_{l, c}^{max}| + |\bar{R}_{l, c}^{-max}|}
\]

$DS_{l, c}$ ranges in $[0, 1]$ and the higher, the more selective channel $c$ in layer $l$ is.
The authors measure the responses of some relevant layers of a pre-trained model for depth estimation.
A graph can be seen in figure \ref{fig:selectivity}.
The mean depth selectivity of the model from which the responses were measured is 0.46.
The mean depth selectivity of a random assignment of depths is 0.3, hence there is some minimal depth selectivity also in models trained without additional constraints.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figs/selectivity}
    \caption{
        Depth selectivity of a trained neural network for depth estimation.
        Histograms of $R_{l,c}^{b}$ for certain channels (called "units" in the picture) and layers (MFF and D are the names of the layers in the considered architecture) from \cite{towards_interpretable}.
        Depth was discretized into 64 bins as can be observed from the horizontal axis.
        Selectivity of each channel-layer pair is also reported.
        Some channels exhibit depth selectivity (top row), while in others it completely misses (bottom row).
        \label{fig:selectivity}
    }
\end{figure}

With the purpose of making depth estimation models more interpretable, You et al. propose a regularization term to be used during training.
The authors discard a simpler approach (that is: to directly increase general depth selectivity during training) in favor of a more stable formula.
After assigning to each channel $c$ of a certain layer $l$ of the feature map a bin $b_{c}$, they define the following:
\[
    \bar{R}_{l, c}^{-b_{c}} = \text{mean}_{b \neq b_{c}} R_{l, c}^{b}
\]
The assignment of bins to channels consists of a simple heuristic: given $C_{l}$ channels and $B$ bins, arbitrarily assign $C_{l} / B$ to each bin.
$B$ is chosen so to divide $C_{l}$.
Finally, the regularization term is given by:
\[
    \mathcal{L}_{reg} =
    \sum_{l \in L} \,
    \sum_{c \in [C_{l}]} \,
        \frac
            {|R_{l, c}^{b_{c}}| - |\bar{R}_{l, c}^{-b_{c}}|}
            {|R_{l, c}^{b_{c}}| + |\bar{R}_{l, c}^{-b_{c}}|}
\]
Where $L$ is the set of layers to which apply the regularization.
After training the network using this additional term, You et al. observe an increase in the depth selectivity of the model both globally, with a mean depth selectivity of 0.83, and also locally.
Comparing the figure \ref{fig:increased_selectivity}  with \ref{fig:selectivity}, it can be seen how the histogram of responses peaks more, implying a higher depth selectivity for that channel-layer pair.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figs/increased_selectivity}
    \caption{
        The content is analogous to that of the figure \ref{fig:selectivity}, but it refers to a model regularized with the introduced term.
        The depth selectivity of the considered channel-layer pairs is evident.
        \label{fig:increased_selectivity}
    }
\end{figure}

\section{Thoughts on Interpretability}
In the fields of philosophy, psychology and cognitive science there are a lot of mature works addressing the problem of what an explanation is and how people evaluate its quality, but most of XAI research is based on an intuitive idea of what constitutes a “good” explanation because seeking explanations as defined by such social sciences would lead to failure \cite{Miller}.
There is an explanatory gap (EG) between natural language and mathematical language expressiveness which undermines the possibility to rigorously define what an explanation is in mathematical terms, necessary for applying it to artificial intelligence systems.
This same gap is also the reason for which training objectives expressed as mathematical functions do not account for the complexity of real-life goals involved in tasks such as driving \cite{Zablocki2022}.
Hence, research in the field of XAI is driven by mathematical heuristics that result in intuitively “good” explanations.
However, humans are known to be highly biased when taking their decisions intuitively rather than through critical analysis \cite{Kahneman}. 

To show how humans are not good at intuitively judging what a good explanation is, consider for instance the family of XAI post-hoc methods in computer vision that produce a saliency map (a.k.a. heat map) as explanation (e.g. GradCAM \cite{GradCAM}).
They are so-called local methods, meaning: given an input image $\mathbf{I}$ and a model $f$, they return an explanation of its specific prediction $y = f(\mathbf{I})$.
This explanation is a map assigning an “importance” value to image pixels.
It is easily visualizable (usually overlaid on the input image $\mathbf{I}$, see figure \ref{fig:grad_cam}) by a human who, by visual inspection, can immediately grasp which parts of the image are supposedly more relevant for that prediction by $f$.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figs/grad_cam}
    \caption{
        Image from \cite{GradCAM}.
        (a) Original image.
        (b) Saliency map obtained with another method.
        (c) Saliency map (with respect to the class "cat") obtained with GradCAM and overlaid onto the original image.
        \label{fig:grad_cam}
    }
\end{figure}

For example if the model solves a classification problem then the highlighted pixels represent “where it looked at” for taking the classification decision.
However, the explanatory gap of above is filled by human cognition.
Indeed, humans perceive images at first sight in a very rich way and decompose it directly into objects, materials, lights, shadows, reflections, geometry \cite{pinker2009mind}.
This implies that when looking at an image with highlighted areas (as it is when visualizing a saliency map) it is difficult to separate the rich conscious experience of the image from the actual information provided by the explanation.
The explanation has a very specific mathematical meaning, but it will look “good” as long as the highlighted area roughly contains reasonable parts of the image involved in humanly justifying the ground truth answer.
For instance, the model classifies an image as a cat and cat pixels are highlighted.
Although this kind of explanation is informative of the model behavior and can be 
effectively used for debugging purposes \cite{molnar2022}, it doesn't increase its interpretability.
It doesn't inform about how the model took its decision but rather which was it. 

All XAI methods suffer from similar problems: they provide seemingly “good” explanations, which appear so because of cognitive biases, but don't actually increase interpretability of the model on which they are applied because of the specific mathematical meaning of their explanations.
This is a consequence of the EG which limits human understanding of complex mathematical artificial intelligence models.
This can be alleviated by making assumptions about the targeted task.
The more assumptions are made when developing a model, the smaller the gap is.
In fact assumptions simplify reality, making it more easily representable in mathematical terms and leading to a better understanding of the resulting AI model.
Human tasks, namely tasks that humans can solve and a machine is asked to do as well, are the most susceptible to the explanatory gap.
Examples of them are: object detection, image segmentation, question answering, speech-to-text, driving.
Depth estimation is, generally speaking, a human task, but, if one thinks more carefully, it probably isn't.
It'll be discussed later.
For solving this kind of tasks, humans use their cognitive abilities and the tasks themselves are defined by those abilities.
Because it's difficult to abstract from their own cognition, when dealing with such tasks humans are more easily biased.
Also, simplifying the problem by making assumptions is hardly ever possible.

\vfill

The methods discussed in the previous section need a review after this discussion.
Let's start from the work of You et al. \cite{towards_interpretable}.
The authors assume that depth selectivity of feature channels is a measure of interpretability without explaining why.
They rather give general analogies with other works where a similar dissection of the network behavior was done, but for different tasks.
The proposed interpretability measure is, in my opinion, arguably a bad measure.
Consider a depth estimation model based on a bin method, in such methods there usually are "bin discriminating layers" (BDLs) that are specialized in classifying the input pixels with a bin label through the activation of a feature map.
Depth selectivity measures how much some layers of the model work like a BDL, so for instance a bin method in correspondence of BDLs would appear to be highly interpretable.
But BDLs are the second-to-last step in depth estimation based on bin methods, where the last is just an aggregation of the bin scores into a depth value.
Hence, for You et al., an interpretable network that performs depth estimation is just a network that solves the task in early layers, i.e. a smaller network.

In "Visualization of Convolutional Neural Networks for Monocular Depth Estimation" \cite{Hu}, Hu et al. define their explainability method (in this case a saliency method) based on its output.
They repeatedly notice that if the mask $\textbf{M}$ is not properly optimized the result will be "unexpected" or not "useful".
The mask is defined in such a way that the explanation is meaningful to humans.
So, while other formulations are mathematically equivalent for what concerns the main optimization objective, the regularization is chosen based on the resulting appearance.
Hence, the authors fall in a circularity.
Notice that their objective is not to output a good-looking image, but to investigate the behavior of the model.
While their visualization method can be useful, it is based on an arbitrary concept of "relevance" for pixels contributing to the prediction.

The first work discussed from Dijk et al. \cite{Dijk} is, in my opinion, a good investigation on what certain neural networks do.
It is not \textit{explaining} their behavior, but it is informative nonetheless and sheds light on various potential biases of models trained on the KITTI dataset.
For what concerns the current discussion I don't feel like criticizing it.

\vfill

\paragraph{Depth estimation and Human Tasks.}
Humans can perceive depth.
Formally (assuming a "formally" is even meaningful), in what sense humans perceive depth?
The following discussion is about personal observations based on introspection.

Humans don't perceive the environment neither in terms of pixels, nor through static entities like images are.
It is difficult to align human depth perception to the formulation of machine depth estimation.
Another aspect to consider is that in certain contexts humans can estimate distances, while in others they cannot.
Also: in certain contexts humans can estimate \textit{relative} distance of objects, while in others they cannot.

This implies that humans cannot annotate whole images with their metric depth map and neither with their relative depth maps.
The only kind of annotation humans can perform is that of DIW dataset \cite{DIW}, where humans were asked to tell which pixel was closer to the camera given another.

One could argue that given a digital image to a human, some software support and enough time, an \textit{accurate} dense depth map could be annotated.
For sure, humans are not able to grasp all the content of an image (particularly \textit{depth} content) by looking at it only once.
More looks are necessary and eyes must explore it also applying some sort of visual reasoning, e.g. visually linking objects to their orthogonal projection on some visible straight line for accurately estimating their depth.
Finally, as previously discussed in this chapter, there have been observed visual cues associated with depth perception in humans.
The role of these cues is still to be clarified since neuroscientific studies did not identify yet a global depth perception process that explicitly uses them.
But when it comes to visually reasoning about an image depth, these cues come handy to computational thinking. 
Nevertheless, assuming humans have the right utility instruments and are properly trained, it still has to be empirically proven that they are able to perform a meaningful depth estimation as machines do pixel-wise.

Hence, it can be safely stated that humans can perceive depth but, right now, they cannot perform depth estimation as formulated in computer vision.
Depth estimation is not a human task.

\vfill

\paragraph{The Necessity of Deep Learning in Depth Estimation.}
Let's rewind a little and go back to the concept of explanatory gap (EG).
A significant aspect of the EG is the fact that it is about the general difference in expressiveness of natural language and mathematical language, thus it is not only about explanations.
In order to clarify this point, consider that, when designing an algorithm for solving a certain \textit{problem formulated in natural language}, a developer represents the entities relevant to the problem and relations between them into mathematical language.
Since the natural language is more expressive than mathematical one, it could happen that some concepts cannot be faithfully translated into mathematical representations (as already noted for training objectives of complex learning algorithms and their explanations).
The intrinsic transparency of an algorithm comes from human interpretability of its passages which are supposed to represent meaningful operations.
Thus, a task involving high-level difficult-to-mathematically-represent requests will always have a certain degree of non interpretability.
This has a fundamental consequence on the whole field of XAI: interpretability methods cannot return arbitrarily good explanations for algorithms solving such tasks.
Equivalently, there is an upper limit to interpretability (hence also to transparency, since necessary to interpretability) for these algorithms.

Depth estimation (DE) suffers from this same problem.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
DE, as claimed above, is not a human task and is formulated in mathematical language.
But, even though humans are not able to solve DE, they \textit{implicitly} add constraints to the desired solution.
These constraints are not explicitly stated and involve the semantic of the scene.
In certain DE settings, like \textit{binocular} depth estimation (BDE), the problem can still be solved using geometrical reasoning (epipolar geometry \cite{multiview} for BDE\footnote{
    There are various situations in which BDE is not solvable using only geometric relations \cite{correspondence}.
    Usually due to reflections, high frequency textures, material transparency and occlusion.
    Refer to \cite{stereo} for a survey on deep learning based techniques.
    Notice, however, that deep learning does not substitute the previous techniques for BDE, it rather enhances them.
    In MDE a complete substitution happened.
}).
But, in the case of \textit{monocular} depth estimation (MDE), the problem is ill-posed.
The solution of an MDE instance must implicitly align with human semantic of the scene that cannot be encoded through mathematical language.
Hence, MDE is subject to the EG consequences on interpretability.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
This undermines the possibility of actually developing a fully interpretable algorithmic MDE (since there is an upper limit to the interpretability of its algorithms).
This implies that a non-interpretable technique \textit{must} be used for solving the task.
So, deep learning, which by now it proved more effective than all other learning techniques, \text{must} be inserted in the depth estimation pipeline.
Deep learning is necessary.

Notice though that, while deep learning is necessary, it is not said that the entire procedure must be an end-to-end DL model.
There is room for complementary procedures, possibly interpretable, that support the learned models.
This is where my personal experimental contribution comes in place.

\section{My approach to Interpretability}
MDE is not a human task and hence humans cannot solve it.
Nevertheless, they perceive scene depth and reason about it.
Since an interpretable algorithm is, by definition, an algorithm which passages are human understandable, an interpretable MDE algorithm must be inspired to how humans reason about scene depth.
As stated before, MDE cannot be solved using a \textit{fully} interpretable procedure (which would also contradict the fact that MDE is not a human task), hence black-box models must be used in the overall pipeline.
By introspection, I observed the following inner facts about scene depth perception (and reasoning) from single image:
\begin{enumerate}
    \item{
        Depth information of what I am looking at is immediately provided;
    }
    \item{
        When I am looking at something, I am not looking at other things, even if close to it;
    }
    \item{
        The eyes continuously move across the image;
    }
    \item{
        The eyes visit more often ambiguous regions;
    }
    \item{
        Sometimes, the eyes move following precise patterns;
    }
    \item{
        Depth perception of things can change throughout the image analysis;
    }
    \item{
        Explicit visual reasoning is performed using eyes movement;
    }
    \item{
        The eyes tend to look at corners (in the computer vision sense, refer to \cite{computer_vision});
    }
    \item{
        When the eyes look at an edge, they tend to follow it in subsequent movements;
    }
    \item{
        When the eyes look at a texture-less region, they tend to move randomly in it.
    }
\end{enumerate}
The non-interpretable component, in my opinion, must be inserted in the immediate (partial) perception of depth of things (point 1 of the bullet list).
By things, I don't mean well-formed objects.
Instead, I refer to places where the eyes stop.
Point 2 sounds tautological, but it is fundamental.
It states that when looking at something, even though other things fall into the visual field, no depth information about them is provided.
Mathematically, it is difficult to express this fact.
The way I choose to give it a formalization is: the reliability of pixels in an atomic depth estimation decreases radially.
In this way, for having a reliable depth map of a full image, multiple "looks" at it are necessary.
This well aligns with the tile-based approaches to MDE.